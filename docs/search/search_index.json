{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to nevarok.com.</p>"},{"location":"contacts/","title":"Contact Us","text":"<p>We value your feedback and are here to assist you. Feel free to reach out to us through any of the following channels:</p> <ul> <li> Email: contact@nevarok.com</li> <li> LinkedIn: Kyrylo Mishakin</li> <li> Discord: NevarokML Discord Server</li> <li> GitHub: NevarokML GitHub Repository</li> <li> YouTube: NevarokML YouTube Channel</li> <li> Unreal Marketplace: NevarokML on Unreal Marketplace</li> <li> Other:<ul> <li>Report a Bug</li> <li>Report a Documentation Issue</li> <li>Request a Change or a Feature</li> </ul> </li> </ul> <p>Please don't hesitate to reach out to us with any questions, suggestions, or inquiries. We'll be happy to assist you!</p> <p>Note</p> <p>The above contact information is provided for NevarokML and its associated platforms. For specific support or inquiries related to Unreal Engine or the Unreal Marketplace, please refer to their respective support channels.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"blog/#test-blog","title":"Test Blog","text":"<p>   2022-09-12 by nevarok </p> <p>Test Blog </p>  Continue reading  <p></p>"},{"location":"blog/posts/test-blog/","title":"Test Blog","text":"<p>Test Blog</p>"},{"location":"contributing/","title":"Community and Support","text":"<p>NevarokML is an actively maintained and constantly evolving project that values contributions from its diverse user base. We welcome bug reports, feature requests, documentation improvements, and community engagement to enhance the plugin's functionality and usability.</p>"},{"location":"contributing/#how-to-contribute","title":"How to contribute","text":"<p>We have streamlined the process for reporting issues, suggesting changes, and engaging with the NevarokML community. Here's how you can contribute:</p>  By actively participating in the NevarokML project, you contribute to its growth and help create a better experience for all users. Join us in shaping the future of machine learning in Unreal Engine."},{"location":"contributing/#something-is-not-working","title":"Something is not working?","text":"<p> Report a bug: If you encounter any issues or unexpected behavior while using NevarokML, create an issue on our bug tracker and provide a detailed description along with steps to reproduce the problem.</p>"},{"location":"contributing/#missing-information-in-our-docs","title":"Missing information in our docs?","text":"<p> Report a docs issue: If you find any gaps or inconsistencies in our documentation, feel free to report them. Your feedback helps us improve the quality and clarity of our documentation.</p>"},{"location":"contributing/#want-to-submit-an-idea","title":"Want to submit an idea?","text":"<p> Request a change: If you have an idea for a new feature, improvement, or change in NevarokML, submit a change request. We value your input and actively review and consider all requests.</p>"},{"location":"contributing/#have-a-question-or-need-help","title":"Have a question or need help?","text":"<p> Ask a question: If you have questions about NevarokML or need assistance, our community discussion board is a great place to seek help. Engage with the community and get answers to your queries.</p>"},{"location":"contributing/#join-our-discord-community","title":"Join our Discord community","text":"<p> Join Discord: We also encourage you to join our Discord community, where you can connect with other NevarokML users, discuss ideas, share knowledge, and collaborate on projects. Our friendly community is always ready to provide support and guidance</p>"},{"location":"contributing/reporting-a-bug/","title":"Reporting a bug","text":"<p>NevarokML is an actively maintained project, and we appreciate your help in identifying and reporting any bugs you encounter. To report a bug, please follow the steps below:</p> <p>We appreciate your efforts in creating a high-quality bug report, and our team will review and address the issue accordingly. Thank you for contributing to the improvement of NevarokML!</p>"},{"location":"contributing/reporting-a-bug/#before-creating-an-issue","title":"Before creating an issue","text":""},{"location":"contributing/reporting-a-bug/#upgrade-to-the-latest-version","title":"Upgrade to the latest version","text":"<ul> <li>Before reporting a bug, ensure that you are using the most recent version of NevarokML. Check for any available updates and upgrade if necessary.</li> </ul>"},{"location":"contributing/reporting-a-bug/#search-for-solutions","title":"Search for solutions","text":"<ul> <li>Prior to creating a bug report, search our documentation, issue tracker, and discussions board to see if the bug has already been reported or if there are any known workarounds or solutions available.</li> </ul>"},{"location":"contributing/reporting-a-bug/#create-a-bug-report","title":"Create a bug report","text":"<p>If you have followed the above steps and still encounter a bug, create an issue in our public issue tracker. Provide a clear and concise description of the bug, along with any relevant links to documentation, discussions, or search results related to the issue.</p>"},{"location":"contributing/reporting-a-bug/#reproduction","title":"Reproduction","text":"<ul> <li>Include a minimal reproduction of the bug, preferably as a .zip file. The reproduction should be small and focused, allowing us to quickly identify and reproduce the issue.</li> </ul>"},{"location":"contributing/reporting-a-bug/#steps-to-reproduce","title":"Steps to reproduce","text":"<ul> <li>Clearly outline the steps required to reproduce the bug using the provided reproduction. Keep the steps concise and easy to follow, ensuring continuity in the process.</li> </ul>"},{"location":"contributing/reporting-a-bug/#checklist","title":"Checklist","text":"<ul> <li>Before submitting the bug report, review the checklist to ensure you have followed all the guidelines and provided all the necessary information.</li> </ul>"},{"location":"contributing/reporting-a-docs-issue/","title":"Reporting a documentation issue","text":"<p>If you have identified an inconsistency, lack of clarity, or room for improvement in our documentation, we appreciate your effort in helping us maintain the quality of our documentation. To report a documentation issue, please follow the steps below:</p>  We appreciate your commitment to improving our documentation. Our team will review the issue and work towards resolving the inconsistency or making the necessary improvements. Thank you for your contribution!"},{"location":"contributing/reporting-a-docs-issue/#report-a-documentation-issue","title":"Report a documentation issue","text":"<p>Please thoroughly read the following guide before creating a new documentation issue, and provide the following information as part of the issue:</p>"},{"location":"contributing/reporting-a-docs-issue/#title","title":"Title","text":"<ul> <li>Create a concise and informative title that accurately describes the documentation issue. Include relevant keywords to facilitate searching in the issue tracker.</li> </ul>"},{"location":"contributing/reporting-a-docs-issue/#description","title":"Description","text":"<ul> <li>Provide a clear and concise summary of the inconsistency or issue you encountered in the documentation. Explain why you believe the documentation should be adjusted and describe the severity of the issue. Keep the description brief and to the point.</li> </ul>"},{"location":"contributing/reporting-a-docs-issue/#related-links","title":"Related links","text":"<ul> <li>Share the links to the specific documentation section that needs adjustment or improvement. If applicable, include any related links or anchor links (permanent links) to other relevant sections. This helps us understand the context and identify areas for improvement.</li> </ul>"},{"location":"contributing/reporting-a-docs-issue/#proposed-change-optional","title":"Proposed change (optional)","text":"<ul> <li>If you have specific ideas or proposals for improving the documentation, you can provide them in this section. It can be a rough outline or a concrete proposal. While this field is optional, your suggestions can be valuable for both maintainers and the community.</li> </ul>"},{"location":"contributing/reporting-a-docs-issue/#checklist","title":"Checklist","text":"<p>Before submitting the issue, review the checklist to ensure you have followed all the guidelines and provided all the necessary information.</p>"},{"location":"contributing/requesting-a-change/","title":"Requesting a change for NevarokML","text":"<p>Thank you for considering submitting a change request for NevarokML. We appreciate your interest in contributing to the project and want to ensure that your proposed change aligns with our guidelines and benefits the community. Before creating a change request, please review the following guidelines to help us understand your idea and evaluate its potential implementation.</p> <p>We look forward to reviewing your change request and working together to improve NevarokML.</p>"},{"location":"contributing/requesting-a-change/#before-creating-an-issue","title":"Before creating an issue","text":"<p>Before investing your time in submitting a change request, please answer the following questions to determine if your idea is suitable for NevarokML and aligns with the project's philosophy and goals:</p>"},{"location":"contributing/requesting-a-change/#purpose-of-the-change","title":"Purpose of the change","text":"<ul> <li>Change requests should focus on suggesting improvements, new features, or addressing existing limitations within NevarokML. It is important to understand that change requests are not intended for reporting bugs, as they require specific information for debugging.</li> </ul>"},{"location":"contributing/requesting-a-change/#source-of-inspiration","title":"Source of inspiration","text":"<ul> <li>If you have seen your idea implemented in other machine learning libraries or frameworks, gather enough information about its implementation. Explain what aspects you like and dislike about the implementation, as this helps us evaluate its potential fit.</li> </ul>"},{"location":"contributing/requesting-a-change/#benefit-for-the-community","title":"Benefit for the community","text":"<ul> <li>NevarokML aims to serve a wide range of users in the machine learning community. When evaluating new ideas, it's essential to consider the potential impact and benefit for other users. Seek input from the community and consider alternative viewpoints to ensure the proposed change benefits a larger audience.</li> </ul>"},{"location":"contributing/requesting-a-change/#issue-template","title":"Issue template","text":"<p>Once you have completed the preliminary work and ensured that your idea meets our requirements, you can proceed with creating a change request using the following guide:</p>"},{"location":"contributing/requesting-a-change/#title","title":"Title","text":"<ul> <li>Create a concise and descriptive title that summarizes your idea in one sentence. The title should reflect the potential impact and benefit for the NevarokML community.</li> </ul>"},{"location":"contributing/requesting-a-change/#context-optional","title":"Context (optional)","text":"<ul> <li>If necessary, provide additional context to help us understand your objectives and the circumstances in which you are using NevarokML. Please refrain from describing the change request in this section.</li> </ul>"},{"location":"contributing/requesting-a-change/#description","title":"Description","text":"<ul> <li>Provide a detailed and clear description of your idea. Focus on explaining why your idea is relevant to NevarokML and how it can improve the library's functionality or address existing limitations. Keep the description concise and avoid over-describing the idea. If you have multiple unrelated ideas, create separate change requests for each.</li> </ul>"},{"location":"contributing/requesting-a-change/#related-links","title":"Related links","text":"<ul> <li>If there are any relevant links to discussions, issues, or documentation sections related to your change request, please include them. This will provide additional context and help us evaluate the feedback and input already provided by the community.</li> </ul>"},{"location":"contributing/requesting-a-change/#use-cases","title":"Use cases","text":"<ul> <li>Explain how your proposed change would work from the perspective of NevarokML users. Describe the expected impact and benefits for not only yourself but also other users. Consider any potential implications on existing functionality and whether the change may break any existing features.</li> </ul>"},{"location":"contributing/requesting-a-change/#visuals-optional","title":"Visuals (optional)","text":"<ul> <li>If you have any visuals, such as diagrams, screenshots, or examples, that can help illustrate your proposed change, please include them in this section. Visuals can provide additional clarity and help maintainers better understand your idea.</li> </ul>"},{"location":"contributing/requesting-a-change/#checklist","title":"Checklist","text":"<p>Thanks for following the change request guide for NevarokML. This section ensures that you have read this guide and have provided all the necessary information for us to review your proposed change. We appreciate your contribution, and we will take it from here.</p>"},{"location":"license/","title":"License Agreements","text":"<p>Welcome to the license agreements page for our products. Here, you can find all the relevant license agreements that govern the use of products and materials. It is important to carefully read and understand these agreements before using any of our product.</p> <p>Note</p> <p>Please note that the license agreements may vary depending on the specific product you are using. To ensure compliance, refer to the corresponding license agreement for the particular product you are interested in.</p>"},{"location":"license/#license-agreements_1","title":"License Agreements","text":"<ul> <li>NevarokML License Agreement</li> </ul>"},{"location":"license/#contact-information","title":"Contact Information","text":"<ul> <li>If you have any questions or concerns regarding the license agreements or any product, please contact us</li> </ul> <p>We strive to provide clear and transparent licensing terms to ensure a positive user experience and compliance with applicable laws and regulations. Thank you for choosing us and our products.</p>"},{"location":"license/nevarok-ml-license/","title":"NevarokML License Agreement","text":"<p>LICENSE AGREEMENT</p> <p>This License Agreement (\"Agreement\") is entered into by and between Kyrylo Mishakin, also known as nevarok (\"Author\"), and the licensee (\"Licensee\") for the usage of the Unreal Engine plugin named \"NevarokML\" (\"Plugin\").</p> <p>Grant of License: 1.1. Author grants Licensee a non-exclusive, non-transferable license to use the Plugin for the Licensee's internal business purposes, subject to the terms and conditions of this Agreement. 1.2. The license granted herein does not include the right to sublicense, distribute, sell, or otherwise make the Plugin available to third parties.</p> <p>Ownership and Intellectual Property: 2.1. The Plugin, including all intellectual property rights, remains the sole and exclusive property of Kyrylo Mishakin. Licensee acknowledges and agrees that no ownership or intellectual property rights are transferred under this Agreement. 2.2. If Licensee suggests any new features, functionality, or performance improvements for the Plugin that Author subsequently incorporates into the Plugin, Licensee hereby grants Author a worldwide, non-exclusive, royalty-free, perpetual right and license to use and incorporate such suggestions into the Plugin. Licensee acknowledges that the Plugin incorporating such new features, functionality, or performance shall be the sole and exclusive property of Author and all such suggestions shall be free from any confidentiality restrictions that might otherwise be imposed upon Author under this Agreement.</p> <p>Restrictions: 3.1. Licensee shall not create derivative works based on the Plugin, except as expressly permitted by applicable law. 3.2. Licensee shall not remove, alter, or obscure any copyright, trademark, or proprietary notices embedded in or displayed by the Plugin. 3.3. Licensee shall not use the Plugin for any illegal or unauthorized purpose, including but not limited to infringing upon any third-party intellectual property rights. 3.4. Licensee shall not distribute or include the external executable provided with the Plugin in any products based on the Plugin.</p> <p>Support and Updates: 4.1. Author shall provide reasonable support to Licensee regarding the functionality and use of the Plugin. However, Author is under no obligation to provide updates, bug fixes, or new versions of the Plugin.</p> <p>Term and Termination: 5.1. This Agreement shall remain in effect unless terminated by either party. Either party may terminate this Agreement immediately upon written notice if the other party breaches any material provision of this Agreement. 5.2. Upon termination, Licensee shall immediately cease all use of the Plugin and delete or destroy any copies of the Plugin in their possession or control.</p> <p>Limitation of Liability: 6.1. In no event shall Kyrylo Mishakin be liable for any indirect, incidental, consequential, or punitive damages arising out of or in connection with the use or inability to use the Plugin, even if Kyrylo Mishakin has been advised of the possibility of such damages.</p> <p>Governing Law and Jurisdiction: 7.1. This Agreement shall be governed by and construed in accordance with the laws of the jurisdiction where Kyrylo Mishakin is located. 7.2. Any disputes arising out of or in connection with this Agreement shall be subject to the exclusive jurisdiction of the courts in the jurisdiction where Kyrylo Mishakin is located.</p> <p>Non-Commercial Use: 8.1. Licensee is permitted to obtain the Plugin for free for the sole purpose of non-commercial use. Non-commercial use includes social media content creation, educational, research, or personal projects that do not generate direct revenue. 8.2. Licensee shall ensure that recipients of the Plugin for non-commercial use are made aware of the terms and conditions of this Agreement and agree to abide by them.</p> <p>Commercial Use and Licensing: 8.3. Any use of the Plugin for commercial purposes, including but not limited to projects, products, or services intended for profit, requires Licensee to obtain a separate commercial license from the Author. 8.4. Commercial use includes any content or applications created or enhanced using the Plugin that are intended for profit, including but not limited to games, simulations, or other interactive experiences. 8.5. Licensee shall contact the Author to inquire about and obtain the appropriate commercial license for such use. 8.6. Licensee acknowledges that any commercial use without a valid commercial license is a breach of this Agreement.</p> <p>Please note that the term \"commercial use\" encompasses any use of the Plugin that is intended to generate direct revenue or financial gain.</p> <p>This addition to the License Agreement clarifies the conditions under which the Plugin can be distributed and used, distinguishing between non-commercial and commercial purposes. Licensees are required to obtain a commercial license for any commercial use of the Plugin, ensuring compliance with the licensing terms.</p> <p>By using the Plugin, Licensee acknowledges that they have read and understood this Agreement and agree to be bound by its terms and conditions.</p> <p>Copyright \u00a9 2023 Kyrylo Mishakin</p>"},{"location":"nevarok-ml/","title":"NevarokML: Machine Learning for Unreal Engine","text":"<p>Welcome to NevarokML, the advanced plugin that brings the power of machine learning to Unreal Engine. NevarokML allows you to seamlessly train machine learning models right from within the Unreal Engine environment using stable-baselines3.</p> <p>With NevarokML, you have access to a wide range of supported algorithms, including PPO, A2C, DDPG, DQN, SAC, and TD3, enabling you to choose the one that best suits your needs. The plugin also supports multi-agent training and multi-environment training, offering flexibility and scalability in your machine learning endeavors.</p> <p>Train your models using NevarokML using either C++ or Blueprints, providing developers with the freedom to utilize their preferred programming approach. Whether you're an experienced coder or a visual scripting enthusiast, NevarokML supports you in creating intelligent systems within Unreal Engine.</p> <p>After training your models, NevarokML allows you to import them into Unreal Engine as NNEModelData, seamlessly integrating them into your game or simulation. These trained models can be deployed and shipped to platforms supported by Unreal's Neural Network Engine (NNE), unlocking new possibilities for intelligent and immersive experiences.</p> <p>NevarokML goes beyond gaming and simulation. It can be a powerful tool for solving game design issues and training intelligent non-player characters (NPCs). Its applications are virtually limitless, opening doors to innovation and creativity in various fields.</p> <p>Explore the endless possibilities of machine learning in Unreal Engine with NevarokML. Start leveraging its capabilities today and unlock a new level of intelligent virtual experiences.</p>"},{"location":"nevarok-ml/documentation/a2c/","title":"NevarokML: A2C Algorithm","text":"<p>The NevarokML plugin integrates the powerful Stable Baselines Advantage Actor Critic (A2C) algorithm into the Unreal Engine environment. A2C is an on-policy actor-critic algorithm that combines the benefits of value-based and policy-based methods.</p>"},{"location":"nevarok-ml/documentation/a2c/#a2c-algorithm-overview","title":"A2C Algorithm Overview","text":"<p>The A2C algorithm, as implemented in NevarokML, utilizes a policy model and value function model to estimate the advantage function. It employs advantage estimation and a discounted cumulative reward to optimize both the policy and value function simultaneously. Here are the key features and parameters of the A2C algorithm used in NevarokML:</p> <ul> <li>owner: Parameter represents the owner of the object, usually the object creating the algorithm.</li> <li>policy (Policy): The policy model to use, such as MlpPolicy, CnnPolicy, etc. It determines the architecture and learning capabilities of the agent's policy network.</li> <li>learningRate (Learning Rate): Set the learning rate for the A2C algorithm. It controls the step size during optimization and affects the convergence speed and stability.</li> <li>nSteps (Number of Steps): Specify the number of steps to run for each environment per update. This determines the size of the rollout buffer and influences the trade-off between bias and variance.</li> <li>gamma (Discount Factor (Gamma)): Set the discount factor that determines the weight of future rewards compared to immediate rewards. It influences the agent's preference for short-term or long-term rewards.</li> <li>gaeLambda (Generalized Advantage Estimation (GAE) Lambda): Specify the trade-off factor between bias and variance for the Generalized Advantage Estimator. It affects how rewards are accumulated over time and influences the agent's value function estimation.</li> <li>entCoef (Entropy Coefficient): Set the entropy coefficient for the loss calculation. It encourages exploration by adding an entropy term to the objective function.</li> <li>vfCoef (Value Function Coefficient): Set the value function coefficient for the loss calculation. It balances the importance of the value function and the policy gradient during optimization.</li> <li>maxGradNorm (Maximum Gradient Norm): Specify the maximum value for gradient clipping. It prevents large updates that could destabilize the training process.</li> <li>rmsPropEps (RMSProp Epsilon): Set the epsilon value used in the RMSProp optimizer. It stabilizes the square root computation in the denominator of the RMSProp update.</li> <li>useRmsProp (Use RMSProp): Choose whether to use RMSProp (default) or Adam as the optimizer.</li> <li>useSde (Use SDE): Enable the use of Generalized State Dependent Exploration (gSDE) instead of action noise exploration.</li> <li>sdeSampleFreq (SDE Sample Frequency): Set the frequency to sample a new noise matrix when using gSDE.</li> <li>normalizeAdvantage (Normalize Advantage): Choose whether to normalize the advantage values during training.</li> <li>verbose (Verbose Level): Control the verbosity level of the training process. Set it to 0 for no output, 1 for info messages, and 2 for debug messages.</li> </ul>"},{"location":"nevarok-ml/documentation/a2c/#api","title":"API","text":"<p>Here is API for A2C algorithm in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Models/NevarokMLBaseAlgorithm.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* A2C(UObject* owner,\nconst ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 7e-4,\nconst int nSteps = 5,\nconst float gamma = 0.99,\nconst float gaeLambda = 1.0,\nconst float entCoef = 0.0,\nconst float vfCoef = 0.5,\nconst float maxGradNorm = 0.5,\nconst float rmsPropEps = 1e-5,\nconst bool useRmsProp = true,\nconst bool useSde = false,\nconst int sdeSampleFreq = -1,\nconst bool normalizeAdvantage = false,\nconst int verbose = 1);\n</code></pre> <p>By setting the appropriate parameter values, you can customize the behavior of the A2C algorithm to suit your specific reinforcement learning problem.</p> <p>For more details on the Stable Baselines A2C algorithm, please refer to the original paper, stable-baselines3 documentation page, and the introduction to A2C guide.</p>"},{"location":"nevarok-ml/documentation/algorithm_api/","title":"NevarokML: UNevarokMLBaseAlgorithm API","text":"<p>The UNevarokMLBaseAlgorithm class represents a base algorithm for reinforcement learning in NevarokML.</p>"},{"location":"nevarok-ml/documentation/algorithm_api/#properties","title":"Properties","text":"<ul> <li><code>_algorithm</code> (<code>ENevarokMLAlgorithm</code>): The type of algorithm.</li> <li><code>_policy</code> (<code>ENevarokMLPolicy</code>): The policy used by the algorithm.</li> <li><code>_learningRate</code> (<code>float</code>): The learning rate for the algorithm.</li> <li><code>_nSteps</code> (<code>int32</code>): The number of steps per batch.</li> <li><code>_batchSize</code> (<code>int32</code>): The batch size.</li> <li><code>_nEpochs</code> (<code>int32</code>): The number of training epochs.</li> <li><code>_gamma</code> (<code>float</code>): The discount factor for future rewards.</li> <li><code>_entCoef</code> (<code>float</code>): The coefficient for the entropy bonus.</li> <li><code>_vfCoef</code> (<code>float</code>): The coefficient for the value function loss.</li> <li><code>_clipRange</code> (<code>float</code>): The clipping range for the policy loss.</li> <li><code>_maxGradNorm</code> (<code>float</code>): The maximum gradient norm for gradient clipping.</li> <li><code>_verbose</code> (<code>int</code>): The verbosity level for logging.</li> <li><code>_gaeLambda</code> (<code>float</code>): The lambda parameter for generalized advantage estimation.</li> <li><code>_useSde</code> (<code>bool</code>): Whether to use state-dependent exploration.</li> <li><code>_sdeSampleFreq</code> (<code>int</code>): The frequency of sampling for state-dependent exploration.</li> <li><code>_rmsPropEps</code> (<code>float</code>): The epsilon value for RMSprop optimizer.</li> <li><code>_useRmsProp</code> (<code>bool</code>): Whether to use RMSprop optimizer.</li> <li><code>_normalizeAdvantage</code> (<code>bool</code>): Whether to normalize advantages.</li> <li><code>_bufferSize</code> (<code>int</code>): The size of the replay buffer.</li> <li><code>_learningStarts</code> (<code>int</code>): The number of steps before starting to learn.</li> <li><code>_tau</code> (<code>float</code>): The soft update coefficient for target networks.</li> <li><code>_gradientSteps</code> (<code>int</code>): The number of gradient steps per update.</li> <li><code>_optimizeMemoryUsage</code> (<code>bool</code>): Whether to optimize memory usage.</li> <li><code>_targetUpdateInterval</code> (<code>int</code>): The interval for updating target networks.</li> <li><code>_explorationFraction</code> (<code>float</code>): The fraction of exploration during training.</li> <li><code>_explorationInitialEps</code> (<code>float</code>): The initial value for exploration epsilon.</li> <li><code>_explorationFinalEps</code> (<code>float</code>): The final value for exploration epsilon.</li> <li><code>_useSdeAtWarmup</code> (<code>bool</code>): Whether to use state-dependent exploration during warm-up phase.</li> <li><code>_policyDelay</code> (<code>int</code>): The number of steps to delay policy updates.</li> <li><code>_targetPolicyNoise</code> (<code>float</code>): The noise added to target policy for TD3 algorithm.</li> <li><code>_targetNoiseClip</code> (<code>float</code>): The range of noise for target policy for TD3 algorithm.</li> <li><code>_trainFreq</code> (<code>int</code>): The frequency of training steps.</li> <li><code>_entCoefAuto</code> (<code>bool</code>): Whether to automatically adjust the entropy coefficient.</li> <li><code>_targetEntropyAuto</code> (<code>bool</code>): Whether to automatically adjust the target entropy.</li> <li><code>_targetEntropy</code> (<code>float</code>): The target entropy for SAC algorithm.</li> </ul>"},{"location":"nevarok-ml/documentation/algorithm_api/#methods","title":"Methods","text":""},{"location":"nevarok-ml/documentation/algorithm_api/#ppo","title":"PPO","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* PPO(UObject* owner, const ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 3e-4, const int nSteps = 2048,\nconst int batchSize = 64, const int nEpochs = 10, const float gamma = 0.99,\nconst float gaeLambda = 0.95, const float clipRange = 0.2,\nconst float entCoef = 0.0, const float vfCoef = 0.5,\nconst float maxGradNorm = 0.5, const bool useSde = false,\nconst int sdeSampleFreq = -1, const int verbose = 1);\n</code></pre> Creates a PPO (Proximal Policy Optimization) algorithm instance with the specified parameters.</p>"},{"location":"nevarok-ml/documentation/algorithm_api/#a2c","title":"A2C","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* A2C(UObject* owner, const ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 7e-4, const int nSteps = 5,\nconst float gamma = 0.99, const float gaeLambda = 1.0,\nconst float entCoef = 0.0, const float vfCoef = 0.5,\nconst float maxGradNorm = 0.5, const float rmsPropEps = 1e-5,\nconst bool useRmsProp = true, const bool useSde = false,\nconst int sdeSampleFreq = -1, const bool normalizeAdvantage = false,\nconst int verbose = 1);\n</code></pre> Creates an A2C (Advantage Actor-Critic) algorithm instance with the specified parameters.</p>"},{"location":"nevarok-ml/documentation/algorithm_api/#ddpg","title":"DDPG","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* DDPG(UObject* owner, const ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 1e-3, const int bufferSize = 1000000,\nconst int learningStarts = 100, const int batchSize = 100,\nconst float tau = 0.005, const float gamma = 0.99,\nconst int trainFreq = 1, const int gradientSteps = -1,\nconst bool optimizeMemoryUsage = false, const int verbose = 1);\n</code></pre> Creates a DDPG (Deep Deterministic Policy Gradient) algorithm instance with the specified parameters.</p>"},{"location":"nevarok-ml/documentation/algorithm_api/#dqn","title":"DQN","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* DQN(UObject* owner, const ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 1e-4, const int bufferSize = 1000000,\nconst int learningStarts = 50000, const int batchSize = 32,\nconst float tau = 1.0, const float gamma = 0.99,\nconst int trainFreq = 4, const int gradientSteps = 1,\nconst bool optimizeMemoryUsage = false,\nconst int targetUpdateInterval = 10000,\nconst float explorationFraction = 0.1,\nconst float explorationInitialEps = 1.0,\nconst float explorationFinalEps = 0.05,\nconst float maxGradNorm = 10, const int verbose = 1);\n</code></pre> Creates a DQN (Deep Q-Network) algorithm instance with the specified parameters.</p>"},{"location":"nevarok-ml/documentation/algorithm_api/#sac","title":"SAC","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* SAC(UObject* owner, const ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 3e-4, const int bufferSize = 1000000,\nconst int learningStarts = 100, const int batchSize = 256,\nconst float tau = 0.005, const float gamma = 0.99,\nconst int trainFreq = 1, const int gradientSteps = 1,\nconst bool optimizeMemoryUsage = false, const bool entCoefAuto = true,\nconst float entCoef = 0.0, const int targetUpdateInterval = 1,\nconst bool targetEntropyAuto = true, const float targetEntropy = 0.0,\nconst bool useSde = false, const int sdeSampleFreq = -1,\nconst bool useSdeAtWarmup = false, const int verbose = 1);\n</code></pre> Creates a SAC (Soft Actor-Critic) algorithm instance with the specified parameters.</p>"},{"location":"nevarok-ml/documentation/algorithm_api/#td3","title":"TD3","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* TD3(UObject* owner, const ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 1e-3, const int bufferSize = 1000000,\nconst int learningStarts = 100, const int batchSize = 100,\nconst float tau = 0.005, const float gamma = 0.99,\nconst int trainFreq = 1, const int gradientSteps = -1,\nconst bool optimizeMemoryUsage = false, const int policyDelay = 2,\nconst float targetPolicyNoise = 0.2,\nconst float targetNoiseClip = 0.5, const int verbose = 1);\n</code></pre> Creates a TD3 (Twin Delayed Deep Deterministic) algorithm instance with the specified parameters.</p>"},{"location":"nevarok-ml/documentation/algorithm_api/#getalgorithmtype","title":"GetAlgorithmType","text":"<p><pre><code>ENevarokMLAlgorithm GetAlgorithmType() const;\n</code></pre> Returns the type of algorithm.</p>"},{"location":"nevarok-ml/documentation/algorithm_api/#getpolicytype","title":"GetPolicyType","text":"<p><pre><code>ENevarokMLPolicy GetPolicyType() const;\n</code></pre> Returns the policy used by the algorithm.</p>"},{"location":"nevarok-ml/documentation/algorithm_api/#tojson","title":"ToJson","text":"<p><pre><code>TSharedPtr&lt;FJsonObject&gt; ToJson() const;\n</code></pre> Converts the algorithm settings to a JSON object.</p>"},{"location":"nevarok-ml/documentation/algorithm_overview/","title":"NevarokML: UNevarokMLBaseAlgorithm","text":"<p>The <code>UNevarokMLBaseAlgorithm</code> class in NevarokML represents a base algorithm for reinforcement learning. It provides a foundation for using various reinforcement learning algorithms, such as PPO, A2C, DDPG, DQN, SAC, and TD3.</p>"},{"location":"nevarok-ml/documentation/algorithm_overview/#overview","title":"Overview","text":"<p>The <code>UNevarokMLBaseAlgorithm</code> class serves as a base class for different reinforcement learning algorithms in NevarokML. It provides common functionality and settings that are shared across these algorithms.</p> <p>Reinforcement learning algorithms enable agents to learn and make decisions based on interaction with an environment. They optimize policies or value functions to maximize rewards or achieve specific goals. The NevarokML plugin offers several algorithms that can be used to train agents in various environments.</p> <p>The <code>UNevarokMLBaseAlgorithm</code> class provides factory methods to create instances of specific algorithms with predefined settings.</p>"},{"location":"nevarok-ml/documentation/algorithm_overview/#api-reference","title":"API Reference","text":"<p>For detailed information on the API of the UNevarokMLBaseAlgorithm class, refer to the following documentation pages:</p> <ul> <li>Properties</li> <li>Methods</li> </ul>"},{"location":"nevarok-ml/documentation/algorithm_overview/#conclusion","title":"Conclusion","text":"<p>The <code>UNevarokMLBaseAlgorithm</code> class forms the backbone of reinforcement learning algorithms in NevarokML. It provides a common interface and shared functionality for implementing different algorithms. By utilizing this class, you can easily configure and customize your reinforcement learning experiments.</p>"},{"location":"nevarok-ml/documentation/binary/","title":"NevarokML: Binary Space","text":"<p>Warning</p> <p>This documentation page is reserved for the upcoming feature: <code>Binary</code> space.</p>"},{"location":"nevarok-ml/documentation/box/","title":"NevarokML: Box Space","text":"<p>The NevarokML plugin provides a <code>Box</code> space implementation, representing a (possibly unbounded) box in R^n. It is commonly used to define the observation space in reinforcement learning environments where the state can have continuous values.</p>"},{"location":"nevarok-ml/documentation/box/#box-space-overview","title":"Box Space Overview","text":"<p>The <code>Box</code> space in NevarokML represents a Cartesian product of n closed intervals in R^n. It can have either an identical bound for each dimension or independent bounds for each dimension. Here are the key features and parameters of the <code>Box</code> space:</p> <ul> <li>owner: Parameter represents the owner of the space object, usually the object creating the space.</li> <li>size (Shape): The shape of the box space.</li> <li>min (Low): The lower bound of the box, specifying the minimum value for each dimension.</li> <li>max (High): The upper bound of the box, specifying the maximum value for each dimension.</li> </ul>"},{"location":"nevarok-ml/documentation/box/#api","title":"API","text":"<p>Here is the API for the <code>Box</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* Box(UObject* owner, FNevarokMLIndex2D size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max)\n</code></pre> <p>To create a <code>Box</code> space, call the <code>Box</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>Box</code> space.</p>"},{"location":"nevarok-ml/documentation/boxdxstack/","title":"NevarokML: BoxDXStack Space","text":"<p>The NevarokML plugin provides a <code>BoxDXStack</code> space implementation, which represents a stack of n-dimensional continuous boxes. This space is useful for tasks where the agent needs to maintain a memory of continuous box values.</p>"},{"location":"nevarok-ml/documentation/boxdxstack/#boxdxstack-space-overview","title":"BoxDXStack Space Overview","text":"<p>The <code>BoxDXStack</code> space in NevarokML extends the functionality of the Box space by adding a stack dimension. It allows you to stack multiple instances of the n-dimensional continuous boxes, creating a memory of continuous box values.</p> <ul> <li>owner: Parameter represents the owner of the space object, usually the object creating the space.</li> <li>size (Shape): The shape of the box space.</li> <li>min (Low): The lower bound of the box, specifying the minimum value for each dimension.</li> <li>max (High): The upper bound of the box, specifying the maximum value for each dimension.</li> <li>stack: The size of the stack. Specifies how many instances of the n-dimensional continuous boxes are stacked.</li> </ul> <p>Note</p> <p>The values are stacked along the X-axis.</p>"},{"location":"nevarok-ml/documentation/boxdxstack/#api","title":"API","text":"<p>Here is the API for the <code>BoxDXStack</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* BoxDXStack(UObject* owner, FNevarokMLIndex2D size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max, int stack = 1);\n</code></pre> <p>To create a <code>BoxDXStack</code> space, call the <code>BoxDXStack</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>BoxDXStack</code> space.</p>"},{"location":"nevarok-ml/documentation/boxdystack/","title":"NevarokML: BoxDYStack Space","text":"<p>The NevarokML plugin provides a <code>BoxDYStack</code> space implementation, which represents a stack of n-dimensional continuous boxes. This space is useful for tasks where the agent needs to maintain a memory of continuous box values.</p>"},{"location":"nevarok-ml/documentation/boxdystack/#boxdystack-space-overview","title":"BoxDYStack Space Overview","text":"<p>The <code>BoxDYStack</code> space in NevarokML extends the functionality of the Box space by adding a stack dimension. It allows you to stack multiple instances of the n-dimensional continuous boxes, creating a memory of continuous box values.</p> <ul> <li>owner: Parameter represents the owner of the space object, usually the object creating the space.</li> <li>size (Shape): The shape of the box space.</li> <li>min (Low): The lower bound of the box, specifying the minimum value for each dimension.</li> <li>max (High): The upper bound of the box, specifying the maximum value for each dimension.</li> <li>stack: The size of the stack. Specifies how many instances of the n-dimensional continuous boxes are stacked.</li> </ul> <p>Note</p> <p>The values are stacked along the Y-axis.</p>"},{"location":"nevarok-ml/documentation/boxdystack/#api","title":"API","text":"<p>Here is the API for the <code>BoxDYStack</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* BoxDYStack(UObject* owner, FNevarokMLIndex2D size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max, int stack = 1);\n</code></pre> <p>To create a <code>BoxDYStack</code> space, call the <code>BoxDYStack</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>BoxDYStack</code> space.</p>"},{"location":"nevarok-ml/documentation/continuous/","title":"NevarokML: Continuous Space","text":"<p>The NevarokML plugin provides a <code>Continuous</code> space implementation, representing a single-dimensional <code>Box</code> space in Stable Baselines3.</p>"},{"location":"nevarok-ml/documentation/continuous/#continuous-space-overview","title":"Continuous Space Overview","text":"<p>The <code>Continuous</code> space in NevarokML corresponds to the <code>Box</code> space in Stable Baselines3. It represents a single-dimensional box in R. The interval can have the form of [a, b] allowing for a wide range of continuous values.</p> <ul> <li>owner: The owner object of the space (usually the object creating the space).</li> <li>size (Number of Elements): The size of the continuous array.</li> <li>min: An array of minimum values specifying the lower bounds for each element.</li> <li>max: An array of maximum values specifying the upper bounds for each element.</li> </ul>"},{"location":"nevarok-ml/documentation/continuous/#api","title":"API","text":"<p>Here is the API for the <code>Continuous</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* Continuous(UObject* owner, int size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max)\n</code></pre> <p>To create a <code>Continuous</code> space, call the <code>Continuous</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>Continuous</code> space.</p>"},{"location":"nevarok-ml/documentation/continuousstack/","title":"NevarokML: ContinuousStack Space","text":"<p>The NevarokML plugin provides a <code>ContinuousStack</code> space implementation, which represents a stack of continuous values. This space is useful for tasks where the agent needs to maintain a memory of continuous values.</p>"},{"location":"nevarok-ml/documentation/continuousstack/#continuousstack-space-overview","title":"ContinuousStack Space Overview","text":"<p>The <code>ContinuousStack</code> space in NevarokML extends the functionality of the Continuous space by adding a stack dimension. It allows you to stack multiple instances of the continuous spaces values, creating a memory of continuous values.</p> <ul> <li>owner: Represents the owner of the space object, usually the object creating the space.</li> <li>size: The size of the continuous space.</li> <li>min: An array of minimum values for each dimension of the continuous space.</li> <li>max: An array of maximum values for each dimension of the continuous space.</li> <li>stack: The size of the stack. Specifies how many instances of the continuous action space are stacked.</li> </ul>"},{"location":"nevarok-ml/documentation/continuousstack/#api","title":"API","text":"<p>Here is the API for the <code>ContinuousStack</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* ContinuousStack(UObject* owner, int size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max, int stack = 1);\n</code></pre> <p>To create a <code>ContinuousStack</code> space, call the <code>ContinuousStack</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>ContinuousStack</code> space.</p>"},{"location":"nevarok-ml/documentation/ddpg/","title":"NevarokML: DDPG (TD3) Algorithm","text":"<p>The NevarokML plugin integrates the Deep Deterministic Policy Gradient (DDPG) algorithm, which is a special case of its successor Twin Delayed DDPG (TD3), into the Unreal Engine environment. DDPG is an off-policy actor-critic algorithm that can handle continuous action spaces. It combines the benefits of the deterministic policy gradient algorithm and the Q-learning algorithm.</p>"},{"location":"nevarok-ml/documentation/ddpg/#ddpg-algorithm-overview","title":"DDPG Algorithm Overview","text":"<p>The DDPG algorithm, as implemented in NevarokML, consists of an actor-critic architecture where the actor is a deterministic policy and the critic estimates the Q-value function. It uses a replay buffer to store and sample experiences for training. Here are the key features and parameters of the DDPG algorithm used in NevarokML:</p> <ul> <li>owner: Parameter represents the owner of the object, usually the object creating the algorithm.</li> <li>policy (Policy): The policy model to use, such as MlpPolicy, CnnPolicy, etc. It determines the architecture and learning capabilities of the agent's policy network.</li> <li>learningRate (Learning Rate): Set the learning rate for the Adam optimizer. The same learning rate will be used for all networks (Q-Values, Actor, and Value function).</li> <li>bufferSize (Replay Buffer Size): Specify the size of the replay buffer, which stores the agent's experiences for training.</li> <li>learningStarts (Learning Starts): Determine how many steps of the model to collect transitions for before learning starts. It ensures that enough experiences are collected before training begins.</li> <li>batchSize (Batch Size): Set the minibatch size for each gradient update. It controls the number of experiences sampled from the replay buffer for each training iteration.</li> <li>tau (Soft Update Coefficient): Set the coefficient for the soft update of the target networks. It determines the interpolation weight between the current and target networks during the update.</li> <li>gamma (Discount Factor): Set the discount factor that determines the weight of future rewards compared to immediate rewards. It influences the agent's preference for short-term or long-term rewards.</li> <li>trainFreq (Train Frequency): Update the model every <code>trainFreq</code> steps.</li> <li>gradientSteps (Gradient Steps): Specify how many gradient steps to perform after each rollout. Set to <code>-1</code> to perform as many gradient steps as steps done in the environment during the rollout.</li> <li>optimizeMemoryUsage (Optimize Memory Usage): Enable a memory-efficient variant of the replay buffer at the cost of increased complexity. See here for more details.</li> <li>verbose (Verbose Level): Control the verbosity level of the training process. Set it to 0 for no output, 1 for info messages, and 2 for debug messages.</li> </ul>"},{"location":"nevarok-ml/documentation/ddpg/#api","title":"API","text":"<p>Here is the API for the DDPG (TD3) algorithm in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Models/NevarokMLBaseAlgorithm.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* DDPG(UObject* owner,\nconst ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 1e-3,\nconst int bufferSize = 1000000,\nconst int learningStarts = 100,\nconst int batchSize = 100,\nconst float tau = 0.005,\nconst float gamma = 0.99,\nconst int trainFreq = 1,\nconst int gradientSteps = -1,\nconst bool optimizeMemoryUsage = false,\nconst int verbose = 1)\n</code></pre> <p>By setting the appropriate parameter values, you can customize the behavior of the DDPG (TD3) algorithm to suit your specific reinforcement learning problem.</p> <p>For more details on the Stable Baselines DDPG algorithm, please refer to the Deterministic Policy Gradient paper, DDPG Paper, stable-baselines3 documentation page, and the introduction to DDPG guide.</p>"},{"location":"nevarok-ml/documentation/discrete/","title":"NevarokML: Discrete Space","text":"<p>The NevarokML plugin provides a <code>Discrete</code> space implementation, representing a discrete space with values in the set <code>{0, 1, ..., n-1}</code>. It is commonly used to define the action space in reinforcement learning environments where the agent can choose from a finite number of actions.</p>"},{"location":"nevarok-ml/documentation/discrete/#discrete-space-overview","title":"Discrete Space Overview","text":"<p>The <code>Discrete</code> space in NevarokML corresponds to a discrete set of values ranging from 0 to n-1. It is used to represent the action space in reinforcement learning environments. Here's the key feature and parameter of the <code>Discrete</code> space:</p> <ul> <li>owner: Parameter represents the owner of the space object, usually the object creating the space.</li> <li>size (N): The size of the discrete space, indicating the number of possible values the space can take.</li> </ul>"},{"location":"nevarok-ml/documentation/discrete/#api","title":"API","text":"<p>Here is the API for the <code>Discrete</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* Discrete(UObject* owner, int64 size = 1)\n</code></pre> <p>To create a <code>Discrete</code> space, call the <code>Discrete</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>Discrete</code> space.</p>"},{"location":"nevarok-ml/documentation/dqn/","title":"NevarokML: DQN Algorithm","text":"<p>The NevarokML plugin integrates the Deep Q-Network (DQN) algorithm into the Unreal Engine environment. DQN is an off-policy algorithm that combines reinforcement learning with deep neural networks to solve complex tasks. It is based on the original DQN paper and subsequent improvements.</p>"},{"location":"nevarok-ml/documentation/dqn/#dqn-algorithm-overview","title":"DQN Algorithm Overview","text":"<p>The DQN algorithm, as implemented in NevarokML, uses a combination of a deep neural network and a replay buffer to approximate the optimal action-value function. It employs techniques such as experience replay and target networks to stabilize the learning process. Here are the key features and parameters of the DQN algorithm used in NevarokML:</p> <ul> <li>owner: Parameter represents the owner of the object, usually the object creating the algorithm.</li> <li>policy (Policy): The policy model to use, such as MlpPolicy, CnnPolicy, etc. It determines the architecture and learning capabilities of the agent's policy network.</li> <li>learningRate (Learning Rate): Set the learning rate for the Adam optimizer. The same learning rate will be used for all networks (Q-Values, Actor, and Value function).</li> <li>bufferSize (Replay Buffer Size): Specify the size of the replay buffer, which stores the agent's experiences for training.</li> <li>learningStarts (Learning Starts): Determine how many steps of the model to collect transitions for before learning starts. It ensures that enough experiences are collected before training begins.</li> <li>batchSize (Batch Size): Set the minibatch size for each gradient update. It controls the number of experiences sampled from the replay buffer for each training iteration.</li> <li>tau (Soft Update Coefficient): Set the coefficient for the soft update of the target networks. It determines the interpolation weight between the current and target networks during the update.</li> <li>gamma (Discount Factor): Set the discount factor that determines the weight of future rewards compared to immediate rewards. It influences the agent's preference for short-term or long-term rewards.</li> <li>trainFreq (Train Frequency): Update the model every <code>trainFreq</code> steps.</li> <li>gradientSteps (Gradient Steps): Specify how many gradient steps to perform after each rollout. Set to <code>-1</code> to perform as many gradient steps as steps done in the environment during the rollout.</li> <li>optimizeMemoryUsage (Optimize Memory Usage): Enable a memory-efficient variant of the replay buffer at the cost of increased complexity. See here for more details.</li> <li>targetUpdateInterval (Target Update Interval): Specify the interval to update the target network. It determines how often the target network is synchronized with the online network.</li> <li>explorationFraction (Exploration Fraction): Set the fraction of the entire training period over which the exploration rate is reduced. It controls the balance between exploration and exploitation.</li> <li>explorationInitialEps (Exploration Initial Epsilon): Set the initial value of the random action probability. It determines the exploration rate at the beginning of the training.</li> <li>explorationFinalEps (Exploration Final Epsilon): Set the final value of the random action probability. It determines the exploration rate at the end of the training.</li> <li>maxGradNorm (Maximum Gradient Norm): Specify the maximum value for gradient clipping. It prevents large updates that could destabilize the training process.</li> <li>verbose (Verbose Level): Control the verbosity level of the training process. Set it to 0 for no output, 1 for info messages, and 2 for debug messages.</li> </ul>"},{"location":"nevarok-ml/documentation/dqn/#api","title":"API","text":"<p>Here is the API for the DQN algorithm in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Models/NevarokMLBaseAlgorithm.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* DQN(UObject* owner,\nconst ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 1e-4,\nconst int bufferSize = 1000000,\nconst int learningStarts = 50000,\nconst int batchSize = 32,\nconst float tau = 1.0,\nconst float gamma = 0.99,\nconst int trainFreq = 4,\nconst int gradientSteps = 1,\nconst bool optimizeMemoryUsage = false,\nconst int targetUpdateInterval = 10000,\nconst float explorationFraction = 0.1,\nconst float explorationInitialEps = 1.0,\nconst float explorationFinalEps = 0.05,\nconst float maxGradNorm = 10,\nconst int verbose = 1)\n</code></pre> <p>By setting the appropriate parameter values, you can customize the behavior of the DQN algorithm to suit your specific reinforcement learning problem.</p> <p>For more details on the Stable Baselines DQN algorithm, please refer to the DQN Paper, stable-baselines3 documentation page, and the Nature paper.</p>"},{"location":"nevarok-ml/documentation/environment_api/","title":"NevarokML: ANevarokMLEnv API","text":"<p>The ANevarokMLEnv class represents an environment actor in NevarokML.</p>"},{"location":"nevarok-ml/documentation/environment_api/#properties","title":"Properties","text":"<ul> <li><code>_actSample</code> (<code>UNevarokMLSample</code>): The action sample for the environment.</li> <li><code>_obsSample</code> (<code>UNevarokMLSample</code>): The observation sample for the environment.</li> <li><code>_reward</code> (<code>float</code>): The accumulated reward for the current step.</li> <li><code>_episodeReward</code> (<code>float</code>): The accumulated reward for the current episode.</li> <li><code>_maxEpisodeReward</code> (<code>float</code>): The maximum episode reward achieved so far.</li> <li><code>_steps</code> (<code>int</code>): The number of steps taken in the current episode.</li> <li><code>_done</code> (<code>bool</code>): Indicates whether the episode is done.</li> </ul>"},{"location":"nevarok-ml/documentation/environment_api/#methods","title":"Methods","text":""},{"location":"nevarok-ml/documentation/environment_api/#executeinit","title":"ExecuteInit","text":"<pre><code>void ANevarokMLEnv::ExecuteInit(UNevarokMLSpace* actSpace, UNevarokMLSpace* obsSpace);\n</code></pre> <p>Executes the initialization process for the environment.</p>"},{"location":"nevarok-ml/documentation/environment_api/#executestep","title":"ExecuteStep","text":"<pre><code>void ANevarokMLEnv::ExecuteStep();\n</code></pre> <p>Executes a step in the environment.</p>"},{"location":"nevarok-ml/documentation/environment_api/#executereset","title":"ExecuteReset","text":"<pre><code>void ANevarokMLEnv::ExecuteReset();\n</code></pre> <p>Resets the environment.</p>"},{"location":"nevarok-ml/documentation/environment_api/#getactsample","title":"GetActSample","text":"<pre><code>UNevarokMLSample* ANevarokMLEnv::GetActSample() const;\n</code></pre> <p>Returns the action sample for the environment.</p>"},{"location":"nevarok-ml/documentation/environment_api/#getobssample","title":"GetObsSample","text":"<pre><code>UNevarokMLSample* ANevarokMLEnv::GetObsSample() const;\n</code></pre> <p>Returns the observation sample for the environment.</p>"},{"location":"nevarok-ml/documentation/environment_api/#getdone","title":"GetDone","text":"<pre><code>bool ANevarokMLEnv::GetDone() const;\n</code></pre> <p>Returns whether the episode is done.</p>"},{"location":"nevarok-ml/documentation/environment_api/#getreward","title":"GetReward","text":"<pre><code>float ANevarokMLEnv::GetReward() const;\n</code></pre> <p>Returns the accumulated reward for the current step.</p>"},{"location":"nevarok-ml/documentation/environment_api/#addreward","title":"AddReward","text":"<pre><code>void ANevarokMLEnv::AddReward(const float value);\n</code></pre> <p>Adds a reward value to the accumulated reward for the current step.</p>"},{"location":"nevarok-ml/documentation/environment_api/#event-methods","title":"Event Methods","text":""},{"location":"nevarok-ml/documentation/environment_api/#oninit","title":"OnInit","text":"<pre><code>UFUNCTION(BlueprintNativeEvent, Category = \"NevarokML|Env\")\nvoid OnInit(UNevarokMLSpace* actSpace, UNevarokMLSpace* obsSpace);\n</code></pre> <p>The <code>OnInit</code> event is triggered when an environment is initialized. It provides an opportunity to perform any necessary setup or customization related to the initialized environment.</p>"},{"location":"nevarok-ml/documentation/environment_api/#onstep","title":"OnStep","text":"<pre><code>UFUNCTION(BlueprintNativeEvent, Category = \"NevarokML|Env\")\nvoid OnStep();\n</code></pre> <p>The <code>OnStep</code> event is triggered when the environment performs a step. It provides an opportunity to respond to the environment's state after the step and perform any necessary actions or calculations.</p>"},{"location":"nevarok-ml/documentation/environment_api/#onreset","title":"OnReset","text":"<pre><code>UFUNCTION(BlueprintNativeEvent, Category = \"NevarokML|Env\")\nvoid OnReset();\n</code></pre> <p>The <code>OnReset</code> event is triggered when the environment resets. It provides an opportunity to handle any necessary actions or logic related to the reset of the environment.</p>"},{"location":"nevarok-ml/documentation/environment_api/#oninit_implementation","title":"OnInit_Implementation","text":"<pre><code>virtual void ANevarokMLEnv::OnInit_Implementation(UNevarokMLSpace* actSpace, UNevarokMLSpace* obsSpace);\n</code></pre> <p>Implementation of the OnInit event method.</p>"},{"location":"nevarok-ml/documentation/environment_api/#onstep_implementation","title":"OnStep_Implementation","text":"<pre><code>virtual void ANevarokMLEnv::OnStep_Implementation();\n</code></pre> <p>Implementation of the OnStep event method.</p>"},{"location":"nevarok-ml/documentation/environment_api/#onreset_implementation","title":"OnReset_Implementation","text":"<p>` virtual void ANevarokMLEnv::OnReset_Implementation(); ```</p> <p>Implementation of the OnReset event method.</p>"},{"location":"nevarok-ml/documentation/environment_overview/","title":"NevarokML: ANevarokMLEnv","text":"<p>The ANevarokMLEnv class represents an environment actor in the NevarokML plugin. It provides an interface for interacting with the environment and collecting observations and rewards during the training process.</p>"},{"location":"nevarok-ml/documentation/environment_overview/#overview","title":"Overview","text":"<p>The ANevarokMLEnv class serves as the interface between the NevarokML plugin and the environment in which an agent is trained. It provides methods for initializing, stepping, and resetting the environment, as well as retrieving observations and rewards.</p>"},{"location":"nevarok-ml/documentation/environment_overview/#key-features-of-the-anevarokmlenv-class","title":"Key features of the ANevarokMLEnv class:","text":"<p>Manages the state of the environment, including initialization, stepping, and resetting. Collects observations from the environment to provide input to the learning process. Returns rewards based on agent actions and environment dynamics. Supports customization and event handling through Blueprint scripting.</p>"},{"location":"nevarok-ml/documentation/environment_overview/#api-reference","title":"API Reference","text":"<p>For detailed information on the API of the <code>ANevarokMLEnv</code> class, refer to the following documentation pages:</p> <ul> <li>Properties</li> <li>Methods</li> <li>Event Methods</li> </ul>"},{"location":"nevarok-ml/documentation/environment_overview/#conclusion","title":"Conclusion","text":"<p>The <code>ANevarokMLEnv</code> class serves as a bridge between the NevarokML plugin and the environment in which an agent is trained. By extending this class and implementing the necessary event methods, you can seamlessly integrate custom environments into the training pipeline. The provided methods and properties allow for easy interaction with the environment and collection of observations and rewards.</p>"},{"location":"nevarok-ml/documentation/experts/","title":"NevarokML: Experts Training","text":"<p>Warning</p> <p>This documentation page is reserved for the upcoming feature: Experts Training.</p>"},{"location":"nevarok-ml/documentation/imitation/","title":"NevarokML: Imitation Learning","text":"<p>Warning</p> <p>This documentation page is reserved for the upcoming feature: Imitation Learning.</p>"},{"location":"nevarok-ml/documentation/multibinary/","title":"NevarokML: MultiBinary Space","text":"<p>The NevarokML plugin provides a <code>MultiBinary</code> space implementation, representing a single-dimensional <code>MultiBinary</code> space in Stable Baselines3.</p>"},{"location":"nevarok-ml/documentation/multibinary/#multibinary-space-overview","title":"MultiBinary Space Overview","text":"<p>The <code>MultiBinary</code> space in NevarokML corresponds to the <code>MultiBinary</code> space in Stable Baselines3. It represents an n-shape binary space, where the argument <code>n</code> defines the number of elements in the space.</p> <ul> <li>owner: The owner object of the space (usually the object creating the space).</li> <li>size (Number of Elements): The size of the multi binary space.</li> </ul>"},{"location":"nevarok-ml/documentation/multibinary/#multibinary-space-creation","title":"MultiBinary Space Creation","text":"<p>To create a <code>MultiBinary</code> space in NevarokML, use the following factory function:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiBinary(UObject* owner, int32 size = 1)\n</code></pre> <p>To create a <code>MultiBinary</code> space, call the <code>MultiBinary</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>MultiBinary</code> space.</p>"},{"location":"nevarok-ml/documentation/multibinary_hidden/","title":"NevarokML: MultiBinary Space","text":"<p>The NevarokML plugin provides a <code>MultiBinary</code> space implementation, representing a multidimensional binary space. It is commonly used to define the observation space in reinforcement learning environments where the state consists of multiple binary values.</p>"},{"location":"nevarok-ml/documentation/multibinary_hidden/#multibinary-space-overview","title":"MultiBinary Space Overview","text":"<p>The <code>MultiBinary</code> space in NevarokML represents a multidimensional binary space, where each element can take a binary value (0 or 1).</p> <ul> <li>owner: Parameter represents the owner of the space object, usually the object creating the space.</li> <li>size (Shape): The shape of the multi binary space.</li> </ul>"},{"location":"nevarok-ml/documentation/multibinary_hidden/#api","title":"API","text":"<p>Here is the API for the <code>MultiBinary</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiBinary(UObject* owner, FNevarokMLIndex2D size)\n</code></pre> <p>To create a <code>MultiBinary</code> space, call the <code>MultiBinary</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>MultiBinary</code> space.</p>"},{"location":"nevarok-ml/documentation/multibinarystack/","title":"NevarokML: MultiBinaryStack Space","text":"<p>The NevarokML plugin provides a <code>MultiBinaryStack</code> space implementation, which represents a stack of multi-binary values. This space is useful for tasks where the agent needs to maintain a memory of binary values.</p>"},{"location":"nevarok-ml/documentation/multibinarystack/#multibinarystack-space-overview","title":"MultiBinaryStack Space Overview","text":"<p>The <code>MultiBinaryStack</code> space in NevarokML extends the functionality of the MultiBinary space by adding a stack dimension. It allows you to stack multiple instances of the multi-binary space values, creating a memory of binary values.</p> <ul> <li>owner: Represents the owner of the space object, usually the object creating the space.</li> <li>size: The size of the binary space.</li> <li>stack: The size of the stack. Specifies how many instances of the multi-binary action space are stacked.</li> </ul>"},{"location":"nevarok-ml/documentation/multibinarystack/#api","title":"API","text":"<p>Here is the API for the <code>MultiBinaryStack</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiBinaryStack(UObject* owner, int32 size = 1, int stack = 1);\n</code></pre> <p>To create a <code>MultiBinaryStack</code> space, call the <code>MultiBinaryStack</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>MultiBinaryStack</code> space.</p>"},{"location":"nevarok-ml/documentation/multidiscrete/","title":"NevarokML: MultiDiscrete Space","text":"<p>The NevarokML plugin provides a <code>MultiDiscrete</code> space implementation, representing a series of discrete action spaces with different numbers of actions in each.</p>"},{"location":"nevarok-ml/documentation/multidiscrete/#multidiscrete-space-overview","title":"MultiDiscrete Space Overview","text":"<p>The <code>MultiDiscrete</code> space in NevarokML is useful for representing game controllers or keyboards where each key can be represented as a discrete action space. It is parametrized by passing an array of positive integers specifying the number of actions for each discrete action space.</p> <ul> <li>owner: Parameter represents the owner of the space object, usually the object creating the space.</li> <li>vec: An array of positive integers specifying the number of actions for each discrete action space.</li> </ul>"},{"location":"nevarok-ml/documentation/multidiscrete/#api","title":"API","text":"<p>Here is the API for the <code>MultiDiscrete</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiDiscrete(UObject* owner, const TArray&lt;int64&gt;&amp; vec)\n</code></pre> <p>To create a <code>MultiDiscrete</code> space, call the <code>MultiDiscrete</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>MultiDiscrete</code> space.</p>"},{"location":"nevarok-ml/documentation/multidiscretestack/","title":"NevarokML: MultiDiscreteStack Space","text":"<p>The NevarokML plugin provides a <code>MultiDiscreteStack</code> space implementation, which represents a stack of multi-discrete action values. This space is useful for tasks where the agent needs to maintain a memory.</p>"},{"location":"nevarok-ml/documentation/multidiscretestack/#multidiscretestack-space-overview","title":"MultiDiscreteStack Space Overview","text":"<p>The <code>MultiDiscreteStack</code> space in NevarokML extends the functionality of the MultiDiscrete space by adding a stack dimension. It allows you to stack multiple instances of the multi-discrete space values, creating a memory of multi-discrete values.</p> <ul> <li>owner: Represents the owner of the space object, usually the object creating the space.</li> <li>vec: An array of positive integers specifying the number of actions for each discrete action space.</li> <li>stack: The size of the stack. Specifies how many values of the multi-discrete action space are stacked.</li> </ul>"},{"location":"nevarok-ml/documentation/multidiscretestack/#api","title":"API","text":"<p>Here is the API for the <code>MultiDiscreteStack</code> space in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Spaces/NevarokMLSpace.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiDiscreteStack(UObject* owner, const TArray&lt;int64&gt;&amp; vec, int stack = 1);\n</code></pre> <p>To create a <code>MultiDiscreteStack</code> space, call the <code>MultiDiscreteStack</code> factory function and provide the required parameters. The function will return an instance of the UNevarokMLSpace class, representing the <code>MultiDiscreteStack</code> space.</p>"},{"location":"nevarok-ml/documentation/nnemodel_api/","title":"NevarokML: UNevarokMLNNEModel API","text":"<p>The <code>UNevarokMLNNEModel</code> acts as a bridge, facilitating the combination of NevarokML's machine learning capabilities with the powerful Unreal Engine Neural Network Engine. By leveraging the UNevarokMLNNEModel, you can seamlessly integrate trained neural network models into your Unreal Engine projects, unlocking advanced AI and machine learning functionalities.</p>"},{"location":"nevarok-ml/documentation/nnemodel_api/#properties","title":"Properties","text":"<ul> <li><code>_cpuModel</code> (<code>TSharedPtr&lt;UE::NNECore::IModelCPU&gt;</code>): The CPU model for the neural network.</li> <li><code>_actSample</code> (<code>UNevarokMLSample</code>): The action sample for the neural network model.</li> <li><code>_obsSample</code> (<code>UNevarokMLSample</code>): The observation sample for the neural network model.</li> <li><code>_inputTensors</code> (<code>TArray&lt;FNevarokMLNNETensor&gt;</code>): The input tensors for the neural network.</li> <li><code>_outputTensors</code> (<code>TArray&lt;FNevarokMLNNETensor&gt;</code>): The output tensors for the neural network.</li> <li><code>_inputBindings</code> (<code>TArray&lt;UE::NNECore::FTensorBindingCPU&gt;</code>): The input bindings for the neural network model.</li> <li><code>_outputBindings</code> (<code>TArray&lt;UE::NNECore::FTensorBindingCPU&gt;</code>): The output bindings for the neural network model.</li> </ul>"},{"location":"nevarok-ml/documentation/nnemodel_api/#methods","title":"Methods","text":""},{"location":"nevarok-ml/documentation/nnemodel_api/#isvalid","title":"IsValid","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|NNEModel\")\nbool UNevarokMLNNEModel::IsValid() const;\n</code></pre> Checks if the neural network model is valid.</p>"},{"location":"nevarok-ml/documentation/nnemodel_api/#predict","title":"Predict","text":"<p><pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|NNEModel\")\nbool UNevarokMLNNEModel::Predict();\n</code></pre> Runs the neural network model prediction and update the action sample based on the observation sample data.</p>"},{"location":"nevarok-ml/documentation/nnemodel_api/#nnemodel","title":"NNEModel","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|NNEModel\")\nstatic UNevarokMLNNEModel* UNevarokMLNNEModel::NNEModel(UObject* owner, UNNEModelData* modelData, UNevarokMLSample* actSample, UNevarokMLSample* obsSample);\n</code></pre> Creates a new instance of the UNevarokMLNNEModel class.</p>"},{"location":"nevarok-ml/documentation/nnemodel_overview/","title":"NevarokML: UNevarokMLNNEModel","text":"<p>The UNevarokMLNNEModel class acts as a bridge between NevarokML and the Unreal Engine Neural Network Engine (NNE). It facilitates the integration of trained neural network models into Unreal Engine projects, enabling advanced AI and machine learning capabilities.</p>"},{"location":"nevarok-ml/documentation/nnemodel_overview/#overview","title":"Overview","text":"<p>The UNevarokMLNNEModel class provides a seamless way to incorporate trained neural network models into your Unreal Engine projects. It allows you to leverage the power of machine learning to enhance the intelligence and behavior of your game characters, NPCs, and other entities.</p> <p>With the NevarokML plugin, you can train neural network models using various machine learning algorithms and frameworks. Once trained, you can export the models and import them into your Unreal Engine project using the UNevarokMLNNEModel class. This integration opens up possibilities for implementing sophisticated AI behaviors, natural language processing, computer vision, and more.</p>"},{"location":"nevarok-ml/documentation/nnemodel_overview/#api-reference","title":"API Reference","text":"<p>For detailed information on the API of the UNevarokMLNNEModel class, refer to the following documentation pages:</p> <ul> <li>Properties</li> <li>Methods</li> </ul>"},{"location":"nevarok-ml/documentation/nnemodel_overview/#conclusion","title":"Conclusion","text":"<p>The UNevarokMLNNEModel class provides a convenient and powerful way to integrate trained neural network models into your Unreal Engine projects. By combining the capabilities of NevarokML and the Unreal Engine Neural Network Engine, you can enhance the intelligence and responsiveness of your virtual environments, creating more immersive and engaging experiences for your users.</p>"},{"location":"nevarok-ml/documentation/ppo/","title":"NevarokML: PPO Algorithm","text":"<p>The NevarokML plugin integrates the powerful Stable Baselines Proximal Policy Optimization (PPO) algorithm into the Unreal Engine environment. PPO is a state-of-the-art reinforcement learning algorithm that has demonstrated excellent performance in a wide range of applications.</p>"},{"location":"nevarok-ml/documentation/ppo/#ppo-algorithm-overview","title":"PPO Algorithm Overview","text":"<p>The PPO algorithm, as implemented in NevarokML, combines the benefits of policy gradient methods with an iterative optimization approach. It leverages a clipped surrogate objective function to ensure stable and efficient training. Here are the key features and parameters of the PPO algorithm used in NevarokML:</p> <ul> <li>owner: Parameter represents the owner of the object, usually the object creating the algorithm.</li> <li>policy (Policy): The policy model to use, such as MlpPolicy, CnnPolicy, etc. It determines the architecture and   learning capabilities of the agent's policy network.</li> <li>learningRate (Learning Rate): Set the learning rate for the PPO algorithm. It controls the step size during   optimization and affects the convergence speed and stability.</li> <li>nSteps (Number of Steps): Specify the number of steps to run for each environment per update. This determines the   size of the rollout buffer and influences the trade-off between bias and variance.</li> <li>batchSize (Batch Size): Set the minibatch size used for training. Larger batch sizes can improve training   stability but require more computational resources.</li> <li>nEpochs (Number of Epochs): Define the number of epochs when optimizing the surrogate loss. Each epoch consists of   multiple update steps on the collected samples.</li> <li>gamma (Discount Factor (Gamma)): Set the discount factor that determines the weight of future rewards compared to   immediate rewards. It influences the agent's preference for short-term or long-term rewards.</li> <li>gaeLambda (Generalized Advantage Estimation (GAE) Lambda): Specify the trade-off factor between bias and variance   for the Generalized Advantage Estimator. It affects how rewards are accumulated over time and influences the agent's   value function estimation.</li> <li>clipRange (Clip Range): Set the clipping parameter for the PPO objective. It restricts the policy update to a   certain range, preventing drastic policy changes.</li> <li>entCoef (Entropy Coefficient): Specify the entropy coefficient for the loss calculation. It encourages exploration   by adding an entropy term to the objective function.</li> <li>vfCoef (Value Function Coefficient): Set the value function coefficient for the loss calculation. It balances the   importance of the value function and the policy gradient during optimization.</li> <li>maxGradNorm (Maximum Gradient Norm): Specify the maximum value for gradient clipping. It prevents large updates   that could destabilize the training process.</li> <li>useSde (Use SDE): Enable the use of Generalized State Dependent Exploration (gSDE) instead of action noise   exploration.</li> <li>sdeSampleFreq (SDE Sample Frequency): Set the frequency to sample a new noise matrix when using gSDE.</li> <li>verbose (Verbose Level): Control the verbosity level of the training process. Set it to 0 for no output, 1 for   info messages, and 2 for debug messages.</li> </ul>"},{"location":"nevarok-ml/documentation/ppo/#api","title":"API","text":"<p>Here is API for PPO algorithm in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Models/NevarokMLBaseAlgorithm.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* PPO(UObject* owner,\nconst ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 3e-4,\nconst int nSteps = 2048,\nconst int batchSize = 64,\nconst int nEpochs = 10,\nconst float gamma = 0.99,\nconst float gaeLambda = 0.95,\nconst float clipRange = 0.2,\nconst float entCoef = 0.0,\nconst float vfCoef = 0.5,\nconst float maxGradNorm = 0.5,\nconst bool useSde = false,\nconst int sdeSampleFreq = -1,\nconst int verbose = 1);\n</code></pre> <p>By setting the appropriate parameter values, you can customize the behavior of the PPO algorithm to suit your specific reinforcement learning problem.</p> <p>For more details on the Stable Baselines PPO algorithm, please refer to the original paper,  stable-baselines3 documentation page and the Spinning Up guide.</p>"},{"location":"nevarok-ml/documentation/reinforcement/","title":"NevarokML: Reinforcement Learning","text":"<p>NevarokML is a powerful machine learning plugin for Unreal Engine that allows developers to implement reinforcement learning (RL) capabilities within their projects. By creating a custom environment and agent, developers can train RL models to optimize agent behavior and accomplish specific tasks. This documentation page will guide you through the process of setting up and performing reinforcement learning using NevarokML in Unreal Engine.</p>"},{"location":"nevarok-ml/documentation/reinforcement/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, make sure you have the NevarokML plugin installed in your Unreal Engine project. Follow the installation instructions to integrate it into your project.</p>"},{"location":"nevarok-ml/documentation/reinforcement/#creating-the-rl-environment","title":"Creating the RL Environment","text":""},{"location":"nevarok-ml/documentation/reinforcement/#creating-a-blueprint-derived-from-anevarokmlenv","title":"Creating a Blueprint Derived from ANevarokMLEnv","text":"<p>To create the RL environment, you need to create a Blueprint class that derives from ANevarokMLEnv. This Blueprint will define the interactions between the agent and the environment.</p> <ol> <li>In the Content Browser, right-click and create a new Blueprint Class.</li> <li>Search for \"NevarokML\" and select \"ANevarokMLEnv\" as the parent class.</li> <li>Name the Blueprint class, e.g., \"BP_BasicEnv,\" and click \"Create.\"</li> </ol> <p> </p> Creation of Blueprint class derived from ANevarokMLEnv"},{"location":"nevarok-ml/documentation/reinforcement/#implementing-oninit-onstep-and-onreset-events","title":"Implementing OnInit, OnStep, and OnReset Events","text":"<p>In the BP_BasicEnv Blueprint, you need to implement the OnInit, OnStep, and OnReset events. These events define how the environment initializes, updates during each time step, and resets when an episode is complete.</p> <ol> <li>Open the BP_BasicEnv Blueprint in the Blueprint Editor.</li> <li>In the Event Graph, create Event Graph nodes for OnInit, OnStep, and OnReset.</li> <li>Implement the functionality for each event based on your RL environment's specific requirements.</li> <li>Use NevarokML API to update observations, perform actions, add rewards, and check for episode completion.</li> </ol> <p> </p> ANevarokMLEnv Event Graph nodes: OnInit, OnStep, and OnReset <p> </p> ANevarokMLEnv Event Graph example"},{"location":"nevarok-ml/documentation/reinforcement/#creating-the-rl-trainer","title":"Creating the RL Trainer","text":""},{"location":"nevarok-ml/documentation/reinforcement/#creating-a-blueprint-derived-from-anevarokmltrainer","title":"Creating a Blueprint Derived from ANevarokMLTrainer","text":"<p>To create the RL trainer, you need to create a Blueprint class that derives from ANevarokMLTrainer. This Blueprint will define the learning process and control the RL environments.</p> <ol> <li>In the Content Browser, right-click and create a new Blueprint Class.</li> <li>Search for \"NevarokML\" and select \"ANevarokMLTrainer\" as the parent class.</li> <li>Name the Blueprint class, e.g., \"BP_BasicTrainer,\" and click \"Create.\"</li> </ol> <p> </p> Creation of Blueprint class derived from ANevarokMLTrainer"},{"location":"nevarok-ml/documentation/reinforcement/#implementing-onconstruct-and-onstart-events","title":"Implementing OnConstruct and OnStart Events","text":"<p>In the BP_BasicTrainer Blueprint, you need to implement the OnConstruct and OnStart events. The OnConstruct event sets up the agent's observation and action spaces, while the OnStart event starts the learning process.</p> <ol> <li>Open the BP_BasicTrainer Blueprint in the Blueprint Editor.</li> <li>In the Event Graph, create Event Graph nodes for OnConstruct and OnStart.</li> <li>In the OnConstruct event, use NevarokML API to update the agent's observation and action spaces according to your needs.</li> <li>In the OnStart event, call the Learn function to initiate the learning process.</li> <li>Use the NevarokML API to create the preferred RL algorithm (PPO, DQN, etc.) and set algorithm parameters.</li> </ol> <p> </p> ANevarokMLTrainer OnConstruct example <p> </p> ANevarokMLTrainer OnStart example"},{"location":"nevarok-ml/documentation/reinforcement/#training-the-rl-agent","title":"Training the RL Agent","text":""},{"location":"nevarok-ml/documentation/reinforcement/#choosing-an-rl-algorithm","title":"Choosing an RL Algorithm","text":"<p>Select the RL algorithm that best fits your RL environment and requirements. NevarokML supports various RL algorithms that can be configured to suit different tasks.</p>"},{"location":"nevarok-ml/documentation/reinforcement/#configuring-the-algorithm","title":"Configuring the Algorithm","text":"<p>Use the NevarokML API to configure the chosen RL algorithm within the BP_BasicTrainer Blueprint. Set the hyperparameters and any other specific configurations needed for the algorithm.</p>"},{"location":"nevarok-ml/documentation/reinforcement/#running-the-learning-process","title":"Running the Learning Process","text":""},{"location":"nevarok-ml/documentation/reinforcement/#placing-the-actors-in-the-level","title":"Placing the Actors in the Level","text":"<p>In the Level, place the BP_BasicTrainer derived Actor and the BP_BasicEnv derived Actor.</p> <ol> <li>Drag and drop the BP_BasicTrainer derived Actor from the Content Browser into the Level.</li> <li>Similarly, place the BP_BasicEnv derived Actor in the Level.</li> <li>Add placed in Level BP_BasicEnv reference to the BP_BasicTrainer.</li> </ol> <p> </p> Referencing BP_BasicEnv in BP_BasicTrainer example"},{"location":"nevarok-ml/documentation/reinforcement/#starting-the-learning-process","title":"Starting the Learning Process","text":"<p>After placing the Actors in the Level, press the Play button to start the learning process. The RL agent will interact with the environment, collect experiences, and update its policy during training.</p> <p> </p> Learning Process example <p> </p> TensorBoard Logging"},{"location":"nevarok-ml/documentation/reinforcement/#importing-the-trained-model","title":"Importing the Trained Model","text":"<p>Upon completion of the training process, you can import the trained RL model using the regular Unreal Engine import flow. This allows you to deploy the trained agent in your Unreal Engine project for testing and deployment.</p>"},{"location":"nevarok-ml/documentation/reinforcement/#conclusion","title":"Conclusion","text":"<p>Congratulations! You have successfully set up and implemented reinforcement learning in Unreal Engine using NevarokML. By creating custom environments and agents and configuring RL algorithms, you can train intelligent agents for various tasks and scenarios.</p> <p>For more in-depth information on NevarokML's features and capabilities, refer to the official NevarokML documentation and additional resources.</p>"},{"location":"nevarok-ml/documentation/reward_utils_api/","title":"NevarokML: UNevarokMLRewardUtils API","text":"<p>The UNevarokMLRewardUtils class provides utility functions for working with rewards in NevarokML.</p>"},{"location":"nevarok-ml/documentation/reward_utils_api/#methods","title":"Methods","text":""},{"location":"nevarok-ml/documentation/reward_utils_api/#boolreward","title":"BoolReward","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|RewardUtils\")\nstatic float BoolReward(float trueReward = 1.0f, float falseReward = -1.0f, bool value = false);\n</code></pre> Calculates the reward based on a boolean value. If the value parameter is true, the function returns trueReward, otherwise it returns falseReward.</p>"},{"location":"nevarok-ml/documentation/reward_utils_api/#curve01reward","title":"Curve01Reward","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|RewardUtils\")\nstatic float Curve01Reward(const UCurveFloat* curve, float value, float min, float max, float rewardMultiplier = 1.0f);\n</code></pre> Calculates the reward based on a value and a curve that maps the range [0, 1] to a reward range. The function evaluates the curve at the normalized value of value within the range [min, max] and multiplies it by rewardMultiplier.</p>"},{"location":"nevarok-ml/documentation/reward_utils_api/#stepscurve01reward","title":"StepsCurve01Reward","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|RewardUtils\")\nstatic float StepsCurve01Reward(const UCurveFloat* curve, int timeSteps, int maxTimeSteps = 100, float rewardMultiplier = 1.0f);\n</code></pre> Calculates the reward based on the progress of a sequence of time steps. The function evaluates the curve at the normalized progress of timeSteps within the range [0, maxTimeSteps] and multiplies it by rewardMultiplier.</p>"},{"location":"nevarok-ml/documentation/reward_utils_api/#stepsdonecurve01reward","title":"StepsDoneCurve01Reward","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|RewardUtils\")\nstatic float StepsDoneCurve01Reward(const UCurveFloat* curve, int timeSteps, int maxTimeSteps = 100, bool isDone = false, float rewardMultiplier = 1.0f);\n</code></pre> Calculates the reward based on the progress of a sequence of time steps and a completion flag. If isDone is true, the function calculates the reward using the StepsCurve01Reward function with the given parameters. If isDone is false, the function returns 0.0f.</p>"},{"location":"nevarok-ml/documentation/reward_utils_api/#dotproductcurvereward","title":"DotProductCurveReward","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|RewardUtils\")\nstatic float DotProductCurveReward(const UCurveFloat* curve, FVector a, FVector b, float rewardMultiplier = 1.0f);\n</code></pre> Calculates the reward based on the dot product between two vectors and a curve that maps the dot product range to a reward range. The function normalizes the input vectors, computes the dot product, evaluates the curve at the dot product value, and multiplies it by rewardMultiplier.</p>"},{"location":"nevarok-ml/documentation/sac/","title":"NevarokML: SAC Algorithm","text":"<p>The NevarokML plugin integrates the Soft Actor-Critic (SAC) algorithm into the Unreal Engine environment. SAC is an off-policy maximum entropy deep reinforcement learning algorithm that combines actor-critic methods with entropy regularization to achieve both exploration and exploitation in learning. It is based on the original SAC paper and subsequent improvements.</p>"},{"location":"nevarok-ml/documentation/sac/#sac-algorithm-overview","title":"SAC Algorithm Overview","text":"<p>The SAC algorithm, as implemented in NevarokML, utilizes a stochastic actor-critic architecture with double Q-targets to estimate the action-value function. It incorporates entropy regularization to encourage exploration and maximize the entropy of the policy distribution. Here are the key features and parameters of the SAC algorithm used in NevarokML:</p> <ul> <li>owner: Parameter represents the owner of the object, usually the object creating the algorithm.</li> <li>policy (Policy): The policy model to use, such as MlpPolicy, CnnPolicy, etc. It determines the architecture and learning capabilities of the agent's policy network.</li> <li>learningRate (Learning Rate): Set the learning rate for the Adam optimizer. The same learning rate will be used for all networks (Q-Values, Actor, and Value function).</li> <li>bufferSize (Replay Buffer Size): Specify the size of the replay buffer, which stores the agent's experiences for training.</li> <li>learningStarts (Learning Starts): Determine how many steps of the model to collect transitions for before learning starts. It ensures that enough experiences are collected before training begins.</li> <li>batchSize (Batch Size): Set the minibatch size for each gradient update. It controls the number of experiences sampled from the replay buffer for each training iteration.</li> <li>tau (Soft Update Coefficient): Set the coefficient for the soft update of the target networks. It determines the interpolation weight between the current and target networks during the update.</li> <li>gamma (Discount Factor): Set the discount factor that determines the weight of future rewards compared to immediate rewards. It influences the agent's preference for short-term or long-term rewards.</li> <li>trainFreq (Train Frequency): Update the model every <code>trainFreq</code> steps.</li> <li>gradientSteps (Gradient Steps): Specify how many gradient steps to perform after each rollout. Set to <code>-1</code> to perform as many gradient steps as steps done in the environment during the rollout.</li> <li>optimizeMemoryUsage (Optimize Memory Usage): Enable a memory-efficient variant of the replay buffer at the cost of increased complexity. See here for more details.</li> <li>entCoefAuto: Set the entropy regularization coefficient. Set it to 'auto' to learn it automatically.</li> <li>entCoef (Entropy Coefficient): Set the entropy regularization coefficient. It controls the trade-off between exploration and exploitation. Value less or equal to '0.0' will set 'entCoef' to 'auto' to learn it automatically, value greater '0.0' will set 'entCoef' to 'auto_{entCoef} if 'entCoefAuto' is set to 'true''.</li> <li>targetUpdateInterval (Target Update Interval): Specify the interval to update the target network. It determines how often the target network is synchronized with the online network.</li> <li>targetEntropyAuto: Set the target entropy when learning the entropy coefficient. Set it to 'auto' to learn it automatically.</li> <li>targetEntropy (Target Entropy): Set the target entropy when learning the entropy coefficient. Ignored if 'targetEntropyAuto' is set to 'true'. Value less or equal to '0.0' will set 'targetEntropy' to 'auto'.</li> <li>useSde (Use Generalized State Dependent Exploration): Enable the use of generalized State Dependent Exploration (gSDE) instead of action noise exploration.</li> <li>sdeSampleFreq (SDE Sample Frequency): Set the frequency to sample a new noise matrix when using gSDE. Set to -1 to sample only at the beginning of the rollout.</li> <li>useSdeAtWarmup (Use SDE at Warmup): Specify whether to use gSDE instead of uniform sampling during the warm-up phase before learning starts.</li> <li>verbose (Verbose Level): Control the verbosity level of the training process. Set it to 0 for no output, 1 for info messages, and 2 for debug messages.</li> </ul>"},{"location":"nevarok-ml/documentation/sac/#api","title":"API","text":"<p>Here is the API for the SAC algorithm in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Models/NevarokMLBaseAlgorithm.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* SAC(UObject* owner,\nconst ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 3e-4,\nconst int bufferSize = 1000000,\nconst int learningStarts = 100,\nconst int batchSize = 256,\nconst float tau = 0.005,\nconst float gamma = 0.99,\nconst int trainFreq = 1,\nconst int gradientSteps = 1,\nconst bool optimizeMemoryUsage = false,\nconst bool entCoefAuto = true,\nconst float entCoef = 0.0,\nconst int targetUpdateInterval = 1,\nconst bool targetEntropyAuto = true,\nconst float targetEntropy = 0.0,\nconst bool useSde = false,\nconst int sdeSampleFreq = -1,\nconst bool useSdeAtWarmup = false,\nconst int verbose = 1)\n</code></pre> <p>By setting the appropriate parameter values, you can customize the behavior of the SAC algorithm to suit your specific reinforcement learning problem.</p> <p>For more details on the Stable Baselines SAC algorithm, please refer to the SAC Paper, stable-baselines3 documentation page and the OpenAI Spinning Up Introduction to SAC.</p>"},{"location":"nevarok-ml/documentation/sample_api/","title":"NevarokML: UNevarokMLSample API","text":"<p>The <code>UNevarokMLSample</code> class in NevarokML is designed to hold various types of values, including discrete, continuous, multi-discrete, multi-binary, and box values. It provides methods to get, set, and validate these values. Additionally, it supports stacking and serialization/deserialization of values.</p>"},{"location":"nevarok-ml/documentation/sample_api/#properties","title":"Properties","text":"<ul> <li><code>_longArray</code> (<code>TArray&lt;int64&gt;</code>): An array of int64 values used for discrete and multi-discrete values.</li> <li><code>_floatArray</code> (<code>TArray&lt;float&gt;</code>): An array of float values used for continuous and box values.</li> <li><code>_byteArray</code> (<code>TArray&lt;int8&gt;</code>): An array of int8 values used for multi-binary values.</li> <li><code>_space</code> (<code>UNevarokMLSpace*</code>): A reference to the associated space object.</li> <li><code>_nneBinding</code> (<code>UE::NNECore::FTensorBindingCPU</code>): An instance of the FTensorBindingCPU class used for NNE compatibility.</li> </ul>"},{"location":"nevarok-ml/documentation/sample_api/#methods","title":"Methods","text":""},{"location":"nevarok-ml/documentation/sample_api/#sample-creation","title":"Sample Creation","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Sample\")\nstatic UNevarokMLSample* Sample(UNevarokMLSpace* owner);\n</code></pre> Creates a new <code>UNevarokMLSample</code> object associated with the specified <code>UNevarokMLSpace</code> owner.</p>"},{"location":"nevarok-ml/documentation/sample_api/#getting-values","title":"Getting Values","text":"<p><pre><code>#include \"Samples/NevarokMLSample.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Sample\")\nbool GetDiscreteValue(int64&amp; value);\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Sample\")\nbool GetContinuousValue(int index, float&amp; value);\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Sample\")\nbool GetMultiDiscreteValue(const int index, int64&amp; value);\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Sample\")\nbool GetMultiBinaryValue(int index, int&amp; value);\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Sample\")\nbool GetBoxValue(FNevarokMLIndex2D index, float&amp; value);\n</code></pre> Retrieves the corresponding values from the <code>UNevarokMLSample</code> object.</p>"},{"location":"nevarok-ml/documentation/sample_api/#setting-values","title":"Setting Values","text":"<pre><code>#include \"Samples/NevarokMLSample.h\"\nUFUNCTION(BlueprintCallable, Category = \"NevarokML|Sample\")\nbool SetDiscreteValue(const int64 value);\nUFUNCTION(BlueprintCallable, Category = \"NevarokML|Sample\")\nbool SetMultiDiscreteValue(const int index, int64 value, const bool multicast);\nUFUNCTION(BlueprintCallable, Category = \"NevarokML|Sample\")\nbool SetMultiBinaryValue(const int index, int value, const bool multicast);\nUFUNCTION(BlueprintCallable, Category = \"NevarokML|Sample\")\nbool SetBoxValue(FNevarokMLIndex2D index, float value, const bool multicast);\nUFUNCTION(BlueprintCallable, Category = \"NevarokML|Sample\")\nbool SetContinuousValue(const int index, float value, const bool multicast);\n</code></pre> <p>Sets the corresponding values in the <code>UNevarokMLSample</code> object. By default, the <code>multicast</code> parameter is set to <code>false</code>. However, if you set <code>multicast</code> to <code>true</code>, all of the corresponding stack values will be set to the provided value.</p>"},{"location":"nevarok-ml/documentation/sample_api/#value-validation","title":"Value Validation","text":"<pre><code>#include \"Samples/NevarokMLSample.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Sample\")\nbool ValidateBoxValue(FNevarokMLIndex2D index) const;\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Sample\")\nbool ValidateDiscreteValue() const;\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Sample\")\nbool ValidateMultiDiscreteValue(const int index) const;\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Sample\")\nbool ValidateMultiBinaryValue(int index) const;\nUFUNCTION(BlueprintPure, Category = \"NevarokML|Sample\")\nbool ValidateContinuousValue(const int index) const;\n</code></pre> <p>Validates the corresponding values in the <code>UNevarokMLSample</code> object.</p>"},{"location":"nevarok-ml/documentation/sample_api/#stack-operations","title":"Stack Operations","text":"<pre><code>#include \"Samples/NevarokMLSample.h\"\nUFUNCTION(BlueprintCallable, Category = \"NevarokML|Sample\")\nvoid StackShift();\nUFUNCTION(BlueprintCallable, Category = \"NevarokML|Sample\")\nvoid Clear();\n</code></pre> <p>Performs stack shifting and clearing operations on the <code>UNevarokMLSample</code> object.</p>"},{"location":"nevarok-ml/documentation/sample_api/#serializationdeserialization","title":"Serialization/Deserialization","text":"<pre><code>#include \"Samples/NevarokMLSample.h\"\nvoid ToJson(const FString&amp; field, const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject) const;\nvoid FromJson(const FString&amp; field, const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject);\nvoid SerializeBox(const FString&amp; field, const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject) const;\nvoid SerializeDiscrete(const FString&amp; field, const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject) const;\nvoid SerializeMultiDiscrete(const FString&amp; field, const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject) const;\nvoid SerializeMultiBinary(const FString&amp; field, const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject) const;\nvoid DeserializeDiscrete(const FString&amp; field, const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject);\nvoid DeserializeBox(const FString&amp; field, const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject);\nvoid DeserializeMultiDiscrete(const FString&amp; field,const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject);\nvoid DeserializeMultiBinary(const FString&amp; field, const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject);\n</code></pre> <p>Provides methods for serializing and deserializing the <code>UNevarokMLSample</code> object.</p>"},{"location":"nevarok-ml/documentation/sample_overview/","title":"NevarokML: UNevarokMLSample","text":"<p>The <code>UNevarokMLSample</code> class represents a sample or observation in the NevarokML plugin. It encapsulates the data associated with a sample, such as discrete values, continuous values, multi-discrete values, multi-binary values, and box values.</p>"},{"location":"nevarok-ml/documentation/sample_overview/#overview","title":"Overview","text":"<p>The <code>UNevarokMLSample</code> class provides methods to interact with the data stored in a sample. It allows you to retrieve and set values for different types of spaces, perform validation checks, clear the sample data, and shift values within a stack.</p> <p>Samples are commonly used in reinforcement learning tasks to represent observations or states of an agent within an environment. They capture the information necessary for training and evaluating machine learning models.</p> <p>The NevarokML plugin offers convenient methods to create samples associated with specific space types. You can access the sample's space object, which provides information about the structure and properties of the space associated with the sample.</p>"},{"location":"nevarok-ml/documentation/sample_overview/#api-reference","title":"API Reference","text":"<p>For detailed information on the API of the <code>UNevarokMLSample class</code>, refer to the following documentation pages:</p> <ul> <li>Properties</li> <li>Methods</li> </ul>"},{"location":"nevarok-ml/documentation/sample_overview/#conclusion","title":"Conclusion","text":"<p>The <code>UNevarokMLSample</code> class serves as a fundamental building block in NevarokML for working with samples or observations. It provides a flexible interface for accessing and modifying the data within a sample, allowing you to seamlessly integrate it into your machine learning workflows.</p>"},{"location":"nevarok-ml/documentation/sample_utils_api/","title":"NevarokML: UNevarokMLSampleUtils API","text":"<p>The <code>UNevarokMLSampleUtils</code> class provides utility functions for working with <code>UNevarokMLSample</code> objects.</p>"},{"location":"nevarok-ml/documentation/sample_utils_api/#methods","title":"Methods","text":""},{"location":"nevarok-ml/documentation/sample_utils_api/#setboxvector","title":"SetBoxVector","text":"<pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|SampleUtils\")\nstatic bool SetBoxVector(UNevarokMLSample* sample, const FNevarokMLIndex2D x, const FNevarokMLIndex2D y, const FNevarokMLIndex2D z, const FVector vector, bool multicast);\n</code></pre> <p>Sets the values in the box stack of the given <code>UNevarokMLSample</code> object at the specified indices <code>(x, y, z)</code> with the provided <code>vector</code>. By default, the <code>multicast</code> parameter is set to <code>false</code>. However, if you set <code>multicast</code> to <code>true</code>, all of the corresponding stack values will be set to the provided <code>vector</code>.</p>"},{"location":"nevarok-ml/documentation/sample_utils_api/#setcontinuousvector","title":"SetContinuousVector","text":"<pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|SampleUtils\")\nstatic bool SetContinuousVector(UNevarokMLSample* sample, const int x, const int y, const int z, const FVector vector, bool multicast);\n</code></pre> <p>Sets the values in the continuous stack of the given <code>UNevarokMLSample</code> object at the specified indices <code>(x, y, z)</code> with the provided <code>vector</code>. By default, the <code>multicast</code> parameter is set to <code>false</code>. However, if you set <code>multicast</code> to <code>true</code>, all of the corresponding stack values will be set to the provided <code>vector</code>.</p>"},{"location":"nevarok-ml/documentation/sample_utils_api/#getboxvector","title":"GetBoxVector","text":"<pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|SampleUtils\")\nstatic bool GetBoxVector(UNevarokMLSample* sample, const FNevarokMLIndex2D&amp; x, const FNevarokMLIndex2D&amp; y, const FNevarokMLIndex2D&amp; z, FVector&amp; vector);\n</code></pre> <p>Retrieves the values from the box stack of the given <code>UNevarokMLSample</code> object at the specified indices <code>(x, y, z)</code> and stores them in the provided <code>vector</code>. Returns <code>true</code> if successful, and <code>false</code> otherwise.</p>"},{"location":"nevarok-ml/documentation/sample_utils_api/#getcontinuousvector","title":"GetContinuousVector","text":"<pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|SampleUtils\")\nstatic bool GetContinuousVector(UNevarokMLSample* sample, int x, int y, int z, FVector&amp; vector);\n</code></pre> <p>Retrieves the values from the continuous stack of the given <code>UNevarokMLSample</code> object at the specified indices <code>(x, y, z)</code> and stores them in the provided <code>vector</code>. Returns <code>true</code> if successful, and <code>false</code> otherwise.</p>"},{"location":"nevarok-ml/documentation/space_api/","title":"NevarokML: UNevarokMLSpace API","text":"<p>The <code>UNevarokMLSpace</code> class represents a space in the NevarokML plugin. It provides various methods to create and manipulate different types of spaces.</p>"},{"location":"nevarok-ml/documentation/space_api/#properties","title":"Properties","text":"<ul> <li><code>_shape</code> (<code>TArray&lt;int32&gt;</code>): An array of integers representing the shape of the space.</li> <li><code>_stack</code> (<code>TArray&lt;int32&gt;</code>): An array of integers representing the stack dimension of the space.</li> <li><code>_n</code> (<code>int64</code>): An integer representing the size of the space.</li> <li><code>_num</code> (<code>int32</code>): An integer representing the number of elements in the space.</li> <li><code>_low</code> (<code>TArray&lt;float&gt;</code>): An array of floats representing the lower bounds of the space.</li> <li><code>_high</code> (<code>TArray&lt;float&gt;</code>): An array of floats representing the upper bounds of the space.</li> <li><code>_nVec</code> (<code>TArray&lt;int64&gt;</code>): An array of integers representing the number of actions for each discrete action space.</li> <li><code>_nneShapes</code> (<code>TArray&lt;UE::NNECore::FTensorShape&gt;</code>): An array of <code>UE::NNECore::FTensorShape</code> objects representing the shapes of the space in the NNECore library.</li> </ul>"},{"location":"nevarok-ml/documentation/space_api/#methods","title":"Methods","text":""},{"location":"nevarok-ml/documentation/space_api/#isvalid","title":"IsValid","text":"<pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nbool IsValid();\n</code></pre> <p>Checks if the space is valid. Returns <code>true</code> if the space is valid, and <code>false</code> otherwise.</p>"},{"location":"nevarok-ml/documentation/space_api/#discrete","title":"Discrete","text":"<pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* Discrete(UObject* owner, int64 size = 1);\n</code></pre> <p>Creates a discrete space. The <code>size</code> parameter specifies the number of discrete actions in the space. Returns an instance of <code>UNevarokMLSpace</code> representing the discrete space.</p>"},{"location":"nevarok-ml/documentation/space_api/#multidiscrete","title":"MultiDiscrete","text":"<pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiDiscrete(UObject* owner, const TArray&lt;int64&gt;&amp; vec);\n</code></pre> <p>Creates a multi-discrete space. The <code>vec</code> parameter is an array of positive integers specifying the number of actions for each discrete action space. Returns an instance of <code>UNevarokMLSpace</code> representing the multi-discrete space.</p>"},{"location":"nevarok-ml/documentation/space_api/#multibinary","title":"MultiBinary","text":"<pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiBinary(UObject* owner, int32 size = 1);\n</code></pre> <p>Creates a multi-binary space. The <code>size</code> parameter specifies the number of binary actions in the space. Returns an instance of <code>UNevarokMLSpace</code> representing the multi-binary space.</p>"},{"location":"nevarok-ml/documentation/space_api/#continuous","title":"Continuous","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* Continuous(UObject* owner, int size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max);\n</code></pre> Creates a continuous space. The <code>size</code> parameter specifies the length of the space. The <code>min</code> and <code>max</code> arrays define the lower and upper bounds for each element of the space. Returns an instance of <code>UNevarokMLSpace</code> representing the continuous space.</p>"},{"location":"nevarok-ml/documentation/space_api/#box","title":"Box","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* Box(UObject* owner, FNevarokMLIndex2D size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max);\n</code></pre> Creates a box space. The <code>size</code> parameter is a 2D index specifying the shape of the box space. The <code>min</code> and <code>max</code> arrays define the lower and upper bounds for each element of the box space. Returns an instance of <code>UNevarokMLSpace</code> representing the box space.</p>"},{"location":"nevarok-ml/documentation/space_api/#multidiscretestack","title":"MultiDiscreteStack","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiDiscreteStack(UObject* owner, const TArray&lt;int64&gt;&amp; vec, int stack = 1);\n</code></pre> Creates a stack of multi-discrete spaces. The vec parameter is an array of positive integers specifying the number of actions for each discrete action space. The stack parameter specifies the size of the stack. Returns an instance of UNevarokMLSpace representing the multi-discrete stack space.</p>"},{"location":"nevarok-ml/documentation/space_api/#multibinarystack","title":"MultiBinaryStack","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* MultiBinaryStack(UObject* owner, int32 size = 1, int stack = 1);\n</code></pre> Creates a stack of multi-binary spaces. The size parameter specifies the number of binary actions in the space. The stack parameter specifies the size of the stack. Returns an instance of UNevarokMLSpace representing the multi-binary stack space.</p>"},{"location":"nevarok-ml/documentation/space_api/#continuousstack","title":"ContinuousStack","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* ContinuousStack(UObject* owner, int size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max, int stack = 1);\n</code></pre> Creates a stack of continuous spaces. The size parameter specifies the dimensionality of the space. The min and max arrays define the lower and upper bounds for each dimension of the space. The stack parameter specifies the size of the stack. Returns an instance of UNevarokMLSpace representing the continuous stack space.</p>"},{"location":"nevarok-ml/documentation/space_api/#boxdxstack","title":"BoxDXStack","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* BoxDXStack(UObject* owner, FNevarokMLIndex2D size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max, int stack = 1);\n</code></pre> Creates a stack of box spaces where the stacking is done along the x-axis. The size parameter is a 2D index specifying the shape of the box space. The min and max arrays define the lower and upper bounds for each element of the box space. The stack parameter specifies the size of the stack. Returns an instance of UNevarokMLSpace representing the box stack space.</p>"},{"location":"nevarok-ml/documentation/space_api/#boxdystack","title":"BoxDYStack","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nstatic UNevarokMLSpace* BoxDYStack(UObject* owner, FNevarokMLIndex2D size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max, int stack = 1);\n</code></pre> Creates a stack of box spaces where the stacking is done along the y-axis. The size parameter is a 2D index specifying the shape of the box space. The min and max arrays define the lower and upper bounds for each element of the box space. The stack parameter specifies the size of the stack. Returns an instance of UNevarokMLSpace representing the box stack space.</p>"},{"location":"nevarok-ml/documentation/space_api/#todiscrete","title":"ToDiscrete","text":"<p><pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|Space\")\nvoid ToDiscrete(int64 size);\n</code></pre> Converts the space to a discrete space with the specified size. Updates the space in-place.</p>"},{"location":"nevarok-ml/documentation/space_api/#tomultidiscrete","title":"ToMultiDiscrete","text":"<p><pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|Space\")\nvoid ToMultiDiscrete(const TArray&lt;int64&gt;&amp; vec);\n</code></pre> Converts the space to a multi-discrete space with the specified vec, which is an array of positive integers specifying the number of actions for each discrete action space. Updates the space in-place.</p>"},{"location":"nevarok-ml/documentation/space_api/#tomultibinary","title":"ToMultiBinary","text":"<p><pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|Space\")\nvoid ToMultiBinary(int32 size);\n</code></pre> Converts the space to a multi-binary space with the specified size. Updates the space in-place.</p>"},{"location":"nevarok-ml/documentation/space_api/#tocontinuous","title":"ToContinuous","text":"<p><pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|Space\")\nvoid ToContinuous(int size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max);\n</code></pre> Converts the space to a continuous space with the specified size, min, and max arrays defining the lower and upper bounds for each dimension of the space. Updates the space in-place.</p>"},{"location":"nevarok-ml/documentation/space_api/#tobox","title":"ToBox","text":"<p><pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|Space\")\nvoid ToBox(FNevarokMLIndex2D size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max);\n</code></pre> Converts the space to a box space with the specified size, min, and max arrays defining the lower and upper bounds for each element of the box space. Updates the space in-place.</p>"},{"location":"nevarok-ml/documentation/space_api/#tomultidiscretestack","title":"ToMultiDiscreteStack","text":"<p><pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|Space\")\nvoid ToMultiDiscreteStack(const TArray&lt;int64&gt;&amp; vec, int stack);\n</code></pre> Converts the space to a stack of multi-discrete spaces with the specified vec, which is an array of positive integers specifying the number of actions for each discrete action space, and stack specifying the size of the stack. Updates the space in-place.</p>"},{"location":"nevarok-ml/documentation/space_api/#tomultibinarystack","title":"ToMultiBinaryStack","text":"<p><pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|Space\")\nvoid ToMultiBinaryStack(int32 size, int stack);\n</code></pre> Converts the space to a stack of multi-binary spaces with the specified size specifying the number of binary actions in the space, and stack specifying the size of the stack. Updates the space in-place.</p>"},{"location":"nevarok-ml/documentation/space_api/#tocontinuousstack","title":"ToContinuousStack","text":"<p><pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|Space\")\nvoid ToContinuousStack(int size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max, int stack);\n</code></pre> Converts the space to a stack of continuous spaces with the specified size specifying the dimensionality of the space, min and max arrays defining the lower and upper bounds for each dimension of the space, and stack specifying the size of the stack. Updates the space in-place.</p>"},{"location":"nevarok-ml/documentation/space_api/#toboxdxstack","title":"ToBoxDXStack","text":"<p><pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|Space\")\nvoid ToBoxDXStack(FNevarokMLIndex2D size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max, int stack);\n</code></pre> Converts the space to a stack of box spaces where the stacking is done along the x-axis. The size parameter is a 2D index specifying the shape of the box space. The min and max arrays define the lower and upper bounds for each element of the box space. The stack parameter specifies the size of the stack. Updates the space in-place.</p>"},{"location":"nevarok-ml/documentation/space_api/#toboxdystack","title":"ToBoxDYStack","text":"<p><pre><code>UFUNCTION(BlueprintCallable, Category = \"NevarokML|Space\")\nvoid ToBoxDYStack(FNevarokMLIndex2D size, const TArray&lt;float&gt;&amp; min, const TArray&lt;float&gt;&amp; max, int stack);\n</code></pre> Converts the space to a stack of box spaces where the stacking is done along the y-axis. The size parameter is a 2D index specifying the shape of the box space. The min and max arrays define the lower and upper bounds for each element of the box space. The stack parameter specifies the size of the stack. Updates the space in-place.</p>"},{"location":"nevarok-ml/documentation/space_api/#gettype","title":"GetType","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category = \"NevarokML|Space\")\nENevarokMLSpaceEnum GetType() const;\n</code></pre> Gets the type of the space. Returns the ENevarokMLSpaceEnum value representing the type of the space.</p>"},{"location":"nevarok-ml/documentation/space_api/#tojson","title":"ToJson","text":"<p><pre><code>TSharedPtr&lt;FJsonObject&gt; ToJson() const;\n</code></pre> Converts the space to a JSON object representation. Returns a shared pointer to the JSON object.</p>"},{"location":"nevarok-ml/documentation/space_api/#isspace","title":"IsSpace","text":"<p><pre><code>static bool IsSpace(const FString&amp; name, ENevarokMLSpaceEnum space);\n</code></pre> Checks if the given space is of the specified type. Returns true if the space is of the specified type, and false otherwise.</p>"},{"location":"nevarok-ml/documentation/space_api/#getname","title":"GetName","text":"<p><pre><code>static const FString&amp; GetName(ENevarokMLSpaceEnum space);\n</code></pre> Gets the name of the space type. Returns the name of the space as a string.</p>"},{"location":"nevarok-ml/documentation/space_api/#gettype_1","title":"GetType","text":"<p><pre><code>static ENevarokMLSpaceEnum GetType(const FString&amp; name);\n</code></pre> Gets the space type from the given name. Returns the ENevarokMLSpaceEnum value representing the space type.</p>"},{"location":"nevarok-ml/documentation/space_overview/","title":"NevarokML: UNevarokMLSpace","text":"<p>The UNevarokMLSpace class represents a space in the NevarokML plugin. It provides various methods for creating, modifying, and inspecting different types of spaces used in machine learning tasks.</p>"},{"location":"nevarok-ml/documentation/space_overview/#overview","title":"Overview","text":"<p>The UNevarokMLSpace class is a base class for different types of spaces, including discrete spaces, multi-discrete spaces, multi-binary spaces, continuous spaces, and box spaces. Spaces define the possible states or actions that an agent can take in a reinforcement learning environment.</p> <p>The NevarokML plugin provides several factory methods to create different types of spaces. These methods include:</p> <ul> <li><code>Discrete</code>: Creates a discrete space with a specified size.</li> <li><code>MultiDiscrete</code>: Creates a multi-discrete space with an array specifying the number of actions for each discrete action space.</li> <li><code>MultiBinary</code>: Creates a multi-binary space with a specified size.</li> <li><code>Continuous</code>: Creates a continuous space with a specified size and bounds.</li> <li><code>Box</code>: Creates a box space with a specified shape and bounds.</li> <li><code>MultiDiscreteStack</code>: Creates a stack of multi-discrete spaces.</li> <li><code>MultiBinaryStack</code>: Creates a stack of multi-binary spaces.</li> <li><code>ContinuousStack</code>: Creates a stack of continuous spaces.</li> <li><code>BoxDXStack</code>: Creates a stack of box spaces along the x-axis.</li> <li><code>BoxDYStack</code>: Creates a stack of box spaces along the y-axis.</li> </ul> <p>The UNevarokMLSpace class provides methods to convert the space to different types, such as converting to discrete, multi-discrete, multi-binary, continuous, and box spaces. These conversion methods allow you to modify the properties of the space and update it in-place.</p> <p>You can also access various properties of the space, such as its shape, stack dimension, size, bounds, and more. The class provides methods to check the validity of the space and retrieve its type.</p>"},{"location":"nevarok-ml/documentation/space_overview/#api-reference","title":"API Reference","text":"<p>For detailed information on the API of the UNevarokMLSpace class, refer to the following documentation pages:</p> <ul> <li>Properties</li> <li>Methods</li> </ul>"},{"location":"nevarok-ml/documentation/space_overview/#conclusion","title":"Conclusion","text":"<p>The UNevarokMLSpace class provides a powerful set of tools for creating and manipulating different types of spaces used in machine learning tasks. With the NevarokML plugin, you can easily define and customize spaces to suit your specific requirements.</p>"},{"location":"nevarok-ml/documentation/td3/","title":"NevarokML: TD3 Algorithm","text":"<p>The NevarokML plugin integrates the Twin Delayed DDPG (TD3) algorithm into the Unreal Engine environment. TD3 is an off-policy actor-critic algorithm that addresses function approximation errors in traditional actor-critic methods. It combines insights from the Deep Deterministic Policy Gradient (DDPG) and Twin Delayed DDPG papers to improve stability and performance.</p>"},{"location":"nevarok-ml/documentation/td3/#td3-algorithm-overview","title":"TD3 Algorithm Overview","text":"<p>The TD3 algorithm, as implemented in NevarokML, utilizes a twin critic architecture and delayed policy updates to improve the learning process. It maintains two Q-value networks to reduce overestimation bias. Here are the key features and parameters of the TD3 algorithm used in NevarokML:</p> <ul> <li>owner: Parameter represents the owner of the object, usually the object creating the algorithm.</li> <li>policy (Policy): The policy model to use, such as MlpPolicy, CnnPolicy, etc. It determines the architecture and learning capabilities of the agent's policy network.</li> <li>learningRate (Learning Rate): Set the learning rate for the Adam optimizer. The same learning rate will be used for all networks (Q-Values, Actor, and Value function).</li> <li>bufferSize (Replay Buffer Size): Specify the size of the replay buffer, which stores the agent's experiences for training.</li> <li>learningStarts (Learning Starts): Determine how many steps of the model to collect transitions for before learning starts. It ensures that enough experiences are collected before training begins.</li> <li>batchSize (Batch Size): Set the minibatch size for each gradient update. It controls the number of experiences sampled from the replay buffer for each training iteration.</li> <li>tau (Soft Update Coefficient): Set the coefficient for the soft update of the target networks. It determines the interpolation weight between the current and target networks during the update.</li> <li>gamma (Discount Factor): Set the discount factor that determines the weight of future rewards compared to immediate rewards. It influences the agent's preference for short-term or long-term rewards.</li> <li>trainFreq (Train Frequency): Update the model every <code>trainFreq</code> steps.</li> <li>gradientSteps (Gradient Steps): Specify how many gradient steps to perform after each rollout. Set to <code>-1</code> to perform as many gradient steps as steps done in the environment during the rollout.</li> <li>optimizeMemoryUsage (Optimize Memory Usage): Enable a memory-efficient variant of the replay buffer at the cost of increased complexity. See here for more details.</li> <li>policyDelay (Policy Delay): Set the number of steps between policy updates. The policy and target networks will only be updated once every policy_delay steps per training step.</li> <li>targetPolicyNoise (Target Policy Noise): Set the standard deviation of Gaussian noise added to the target policy (smoothing noise).</li> <li>targetNoiseClip (Target Noise Clip): Set the limit for the absolute value of the target policy smoothing noise.</li> <li>verbose (Verbosity Level): Set the verbosity level for the algorithm's output. Use 0 for no output, 1 for info messages, and 2 for debug messages.</li> </ul>"},{"location":"nevarok-ml/documentation/td3/#api","title":"API","text":"<p>Here is the API for the TD3 algorithm in NevarokML, along with the corresponding default parameter settings:</p> <pre><code>#include \"Models/NevarokMLBaseAlgorithm.h\"\nUFUNCTION(BlueprintPure, Category = \"NevarokML|BaseAlgorithm\")\nstatic UNevarokMLBaseAlgorithm* TD3(UObject* owner,\nconst ENevarokMLPolicy policy = ENevarokMLPolicy::MLP_POLICY,\nconst float learningRate = 1e-3,\nconst int bufferSize = 1000000,\nconst int learningStarts = 100,\nconst int batchSize = 100,\nconst float tau = 0.005,\nconst float gamma = 0.99,\nconst int trainFreq = 1,\nconst int gradientSteps = -1,\nconst bool optimizeMemoryUsage = false,\nconst int policyDelay = 2,\nconst float targetPolicyNoise = 0.2,\nconst float targetNoiseClip = 0.5,\nconst int verbose = 1)\n</code></pre> <p>By setting the appropriate parameter values, you can customize the behavior of the TD3 algorithm to suit your specific reinforcement learning problem.</p> <p>For more details on the Stable Baselines TD3 algorithm, please refer to the TD3 Paper, stable-baselines3 documentation page and the OpenAI Spinning Up Introduction to TD3.</p>"},{"location":"nevarok-ml/documentation/testing-and-deployment/","title":"NevarokML: Testing and Deployment","text":"<p>After training a reinforcement learning model using NevarokML, you can test and deploy the trained model within Unreal Engine. This process involves creating an instance of UNevarokMLNNEModel and using it to predict actions based on observations. In this section, we'll walk through the steps to perform testing and deployment using NevarokML.</p>"},{"location":"nevarok-ml/documentation/testing-and-deployment/#testing-and-deployment-workflow","title":"Testing and Deployment Workflow","text":"<ol> <li> <p>Create an instance of UNevarokMLNNEModel: To start, create an instance of the UNevarokMLNNEModel class. This class represents the trained model and provides functionality for prediction.</p> </li> <li> <p>Provide Action and Observation Samples: You'll need to provide instances of UNevarokMLSample that match the actual model's action space and observation space structure and types. These samples will store the observation and action data that the model will use for prediction.</p> </li> <li> <p>Set UNNEModelData Asset Reference: Provide a reference to the saved model asset (of type UNNEModelData) to the UNevarokMLNNEModel instance.</p> </li> <li> <p>Update Observation Sample: Before calling the Predict() function, update the observation sample with the latest observation data.</p> </li> <li> <p>Call Predict() Function: Call the Predict() function of the UNevarokMLNNEModel instance to update the action sample based on the observation.</p> </li> <li> <p>Use Predicted Actions: You can now use the predicted actions from the action sample wherever you need to perform actions based on your model's decisions.</p> </li> </ol>"},{"location":"nevarok-ml/documentation/testing-and-deployment/#conclusion","title":"Conclusion","text":"<p>Testing and deploying the trained reinforcement learning model within Unreal Engine using NevarokML involves creating an instance of UNevarokMLNNEModel, providing action and observation samples, setting the model data asset reference, updating the observation sample, and calling the Predict() function. This enables you to use the trained model to make predictions and perform actions based on its decisions.</p>"},{"location":"nevarok-ml/documentation/trainer_api/","title":"NevarokML: ANevarokMLTrainer API","text":"<p>The ANevarokMLTrainer class represents a trainer actor in NevarokML.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#properties","title":"Properties","text":"<ul> <li><code>_state</code> (<code>ENevarokMLState</code>): The state of the trainer.</li> <li><code>_tickTimer</code> (<code>float</code>): The timer for trainer ticks.</li> <li><code>_trainerTickFunction</code> (<code>FNevarokMLTrainerTickFunction</code>): The tick function for the trainer.</li> <li><code>_procHandle</code> (<code>TUniquePtr&lt;FProcHandle&gt;</code>): The process handle for the backend.</li> <li><code>_autoKillBackend</code> (<code>bool</code>): Whether to automatically kill the backend process.</li> <li><code>_actSpace</code> (<code>UNevarokMLSpace*</code>): The action space for the trainer.</li> <li><code>_obsSpace</code> (<code>UNevarokMLSpace*</code>): The observation space for the trainer.</li> <li><code>_socketServer</code> (<code>UNevarokMLSocketServer*</code>): The socket server for communication.</li> <li><code>_address</code> (<code>FString</code>): The address for the socket server.</li> <li><code>_port</code> (<code>int32</code>): The port for the socket server.</li> <li><code>_maxEnvsCount</code> (<code>int</code>): The maximum number of environments.</li> <li><code>_tickInterval</code> (<code>float</code>): The interval between ticks.</li> <li><code>_envUpdatesPerTick</code> (<code>int32</code>): The number of environment updates per tick.</li> <li><code>_envs</code> (<code>TArray&lt;ANevarokMLEnv*&gt;</code>): The array of environments.</li> </ul>"},{"location":"nevarok-ml/documentation/trainer_api/#methods","title":"Methods","text":""},{"location":"nevarok-ml/documentation/trainer_api/#anevarokmltrainer","title":"ANevarokMLTrainer","text":"<p><pre><code>ANevarokMLTrainer();\n</code></pre> Constructor for the ANevarokMLTrainer class.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#getenvscount","title":"GetEnvsCount","text":"<p><pre><code>UFUNCTION(BlueprintPure, Category=\"NevarokML|Trainer\")\nint GetEnvsCount() const;\n</code></pre> Returns the number of environments.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#runbackend","title":"RunBackend","text":"<p><pre><code>bool ANevarokMLTrainer::RunBackend();\n</code></pre> Runs the backend process.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#killbackend","title":"KillBackend","text":"<p><pre><code>void ANevarokMLTrainer::KillBackend();\n</code></pre> Kills the backend process.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#validatespaces","title":"ValidateSpaces","text":"<p><pre><code>bool ANevarokMLTrainer::ValidateSpaces() const;\n</code></pre> Validates the action and observation spaces.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#validateenvs","title":"ValidateEnvs","text":"<p><pre><code>bool ANevarokMLTrainer::ValidateEnvs() const;\n</code></pre> Validates the environments.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#parsedata","title":"ParseData","text":"<p><pre><code>bool ANevarokMLTrainer::ParseData(const TArray&lt;uint8&gt;&amp; data, ENevarokMLData&amp; dataType);\n</code></pre> Parses the received data and determines the data type.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#handlereset","title":"HandleReset","text":"<p><pre><code>bool ANevarokMLTrainer::HandleReset(const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject);\n</code></pre> Handles the reset action received from the environment.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#handleready","title":"HandleReady","text":"<p><pre><code>bool ANevarokMLTrainer::HandleReady(const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject);\n</code></pre> Handles the ready event received from the environment.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#handlecomplete","title":"HandleComplete","text":"<p><pre><code>bool ANevarokMLTrainer::HandleComplete(const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject);\n</code></pre> Handles the complete event received from the environment.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#handleerror","title":"HandleError","text":"<p><pre><code>bool ANevarokMLTrainer::HandleError(const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject);\n</code></pre> Handles the error event received from the environment.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#handlesave","title":"HandleSave","text":"<p><pre><code>bool ANevarokMLTrainer::HandleSave(const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject) const;\n</code></pre> Handles the save event received from the environment.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#handleaction","title":"HandleAction","text":"<p><pre><code>bool ANevarokMLTrainer::HandleAction(const TSharedPtr&lt;FJsonObject&gt;&amp; jsonObject);\n</code></pre> Handles the action received from the environment.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#executeinit","title":"ExecuteInit","text":"<p><pre><code>void ANevarokMLTrainer::ExecuteInit();\n</code></pre> Executes the initialization process for the trainer.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#killsocket","title":"KillSocket","text":"<p><pre><code>void ANevarokMLTrainer::KillSocket();\n</code></pre> Kills the socket server.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#executeinvalid","title":"ExecuteInvalid","text":"<p><pre><code>void ANevarokMLTrainer::ExecuteInvalid();\n</code></pre> Executes the invalid state process for the trainer.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#executeinitenv","title":"ExecuteInitEnv","text":"<p><pre><code>void ANevarokMLTrainer::ExecuteInitEnv(int index, ANevarokMLEnv* env);\n</code></pre> Executes the initialization process for an environment.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#executestepenv","title":"ExecuteStepEnv","text":"<p><pre><code>void ANevarokMLTrainer::ExecuteStepEnv(int index, ANevarokMLEnv* env);\n</code></pre> Executes the step process for an environment.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#executeresetenv","title":"ExecuteResetEnv","text":"<p><pre><code>void ANevarokMLTrainer::ExecuteResetEnv(int index, ANevarokMLEnv* env);\n</code></pre> Executes the reset process for an environment.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#ticktrainer","title":"TickTrainer","text":"<p><pre><code>void ANevarokMLTrainer::TickTrainer(float deltaTime);\n</code></pre> Called every frame to update the trainer.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#learn","title":"Learn","text":"<p><pre><code>UFUNCTION(BlueprintCallable, Category=\"NevarokML|Trainer\")\nbool Learn(const UNevarokMLBaseAlgorithm* algorithm,\nint timesteps = 10000,\nint evalEps = 0,\nint saveFreq = 6000,\nconst FFilePath loadModelPath = FFilePath(),\nconst FName saveModelName = FName(\"Model\"),\nconst bool deterministic = true,\nconst bool showTensorboard = false,\nconst bool showReward = false,\nconst bool showStepDebug = false,\nconst bool showResetDebug = true);\n</code></pre> Starts the learning process with the specified algorithm and parameters.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#method-parameters","title":"Method Parameters","text":"<ul> <li><code>algorithm</code> (<code>UNevarokMLBaseAlgorithm*</code>): A pointer to the algorithm object that defines the learning algorithm to be used.</li> <li><code>timesteps</code> (<code>int</code>): The maximum number of timesteps to run the learning process. The default value is 10,000.</li> <li><code>evalEps</code> (<code>int</code>): The number of evaluation episodes to run during the learning process. The default value is 0, indicating no evaluation episodes.</li> <li><code>saveFreq</code> (<code>int</code>): The frequency, in timesteps, at which the model should be saved during the learning process. The default value is 6,000.</li> <li><code>loadModelPath</code> (<code>FFilePath</code>): The file path to a pre-trained model that should be loaded before starting the learning process. The default value is an empty file path, indicating no pre-trained model should be loaded.</li> <li><code>saveModelName</code> (<code>FName</code>): The name to use when saving the trained model. The default value is \"Model\".</li> <li><code>deterministic</code> (<code>bool</code>): A flag indicating whether to use deterministic behavior during the learning process. The default value is true.</li> <li><code>showTensorboard</code> (<code>bool</code>): A flag indicating whether to show Tensorboard visualizations during the learning process. The default value is false.</li> <li><code>showReward</code> (<code>bool</code>): A flag indicating whether to show reward comparison(before and after) information after the learning process is complete. The default value is false.</li> <li><code>showStepDebug</code> (<code>bool</code>): A flag indicating whether to show step debugging information during the learning process. The default value is false.</li> <li><code>showResetDebug</code> (<code>bool</code>): A flag indicating whether to show reset debugging information during the learning process. The default value is true.</li> </ul>"},{"location":"nevarok-ml/documentation/trainer_api/#override-methods","title":"Override Methods","text":""},{"location":"nevarok-ml/documentation/trainer_api/#beginplay","title":"BeginPlay","text":"<p><pre><code>virtual void ANevarokMLTrainer::BeginPlay() override;\n</code></pre> Overrides the BeginPlay function.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#begindestroy","title":"BeginDestroy","text":"<p><pre><code>virtual void ANevarokMLTrainer::BeginDestroy() override;\n</code></pre> Overrides the BeginDestroy function.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#registeractortickfunctions","title":"RegisterActorTickFunctions","text":"<p><pre><code>void ANevarokMLTrainer::RegisterActorTickFunctions(bool bRegister) override;\n</code></pre> Overrides the RegisterActorTickFunctions function.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#event-methods","title":"Event Methods","text":"<p>Event methods for various trainer states and actions. These methods can be overridden in Blueprints for custom implementation.</p> <pre><code>graph LR\n  A[/OnConstruct/] --&gt; OnStart\n  A --&gt;|Error| B((OnInvalid))\n  OnStart --&gt;|Error| B\n  OnStart --&gt;|Learn| C[/OnInit/]\n  C --&gt; E{OnReset}\n  E --&gt; D{OnStep}\n  D --&gt;|Error| B\n  D --&gt;|Not Done| D\n  D --&gt;|Done| E\n  E --&gt; Z((OnComplete))</code></pre>"},{"location":"nevarok-ml/documentation/trainer_api/#onconstruct","title":"OnConstruct","text":"<pre><code>UFUNCTION(BlueprintNativeEvent, Category=\"NevarokML|Trainer\")\nvoid OnConstruct(UNevarokMLSpace* actSpace, UNevarokMLSpace* obsSpace);\n</code></pre> <p>The <code>OnConstruct</code> event serves as a setup step before the training process begins, allowing to configure the action and observation spaces.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#onstart","title":"OnStart","text":"<pre><code>UFUNCTION(BlueprintNativeEvent, Category=\"NevarokML|Trainer\")\nvoid OnStart();\n</code></pre> <p>The <code>OnStart</code> event is triggered after the construction of the trainer object is finished and the initialization is successful. It provides a suitable location to start the learning process by invoking the <code>Learn</code> function with the desired parameters.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#oninvalid","title":"OnInvalid","text":"<pre><code>UFUNCTION(BlueprintNativeEvent, Category=\"NevarokML|Trainer\")\nvoid OnInvalid();\n</code></pre> <p>The <code>OnInvalid</code> event is triggered when there are errors or issues encountered during the construction or learning phases of the trainer object. It serves as a notification and allows you to handle and respond to these errors in a customized manner.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#oninit","title":"OnInit","text":"<pre><code>UFUNCTION(BlueprintNativeEvent, Category=\"NevarokML|Trainer\")\nvoid OnInit(int index, ANevarokMLEnv* env);\n</code></pre> <p>The <code>OnInit</code> event is triggered when an environment at a specific index is successfully initialized. It provides an opportunity to perform any necessary setup or customization related to the initialized environment.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#onstep","title":"OnStep","text":"<pre><code>UFUNCTION(BlueprintNativeEvent, Category=\"NevarokML|Trainer\")\nvoid OnStep(int index, ANevarokMLEnv* env);\n</code></pre> <p>The <code>OnStep</code> event is triggered when the trainer performs a step on the environment at a specific index. It provides an opportunity to respond to the environment's state after the step and perform any necessary actions or calculations.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#onreset","title":"OnReset","text":"<pre><code>UFUNCTION(BlueprintNativeEvent, Category=\"NevarokML|Trainer\")\nvoid OnReset(int index, ANevarokMLEnv* env);\n</code></pre> <p>The <code>OnReset</code> event is triggered when the trainer resets the environment at a specific index. It provides an opportunity to handle any necessary actions or logic related to the reset of the environment.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#oncomplete","title":"OnComplete","text":"<pre><code>UFUNCTION(BlueprintNativeEvent, Category=\"NevarokML|Trainer\")\nvoid OnComplete();\n</code></pre> <p>The <code>OnComplete</code> event is triggered when the training process reaches its completion. It serves as a notification that the training has finished and provides an opportunity to perform any necessary cleanup or additional actions.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#onconstruct_implementation","title":"OnConstruct_Implementation","text":"<p><pre><code>virtual void ANevarokMLTrainer::OnConstruct_Implementation(UNevarokMLSpace* actSpace, UNevarokMLSpace* obsSpace);\n</code></pre> Implementation of the OnConstruct event method.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#onstart_implementation","title":"OnStart_Implementation","text":"<p><pre><code>virtual void ANevarokMLTrainer::OnStart_Implementation();\n</code></pre> Implementation of the OnStart event method.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#oninvalid_implementation","title":"OnInvalid_Implementation","text":"<p><pre><code>virtual void ANevarokMLTrainer::OnInvalid_Implementation();\n</code></pre> Implementation of the OnInvalid event method.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#oninit_implementation","title":"OnInit_Implementation","text":"<p><pre><code>virtual void ANevarokMLTrainer::OnInit_Implementation(int index, ANevarokMLEnv* env);\n</code></pre> Implementation of the OnInit event method.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#onstep_implementation","title":"OnStep_Implementation","text":"<p><pre><code>virtual void ANevarokMLTrainer::OnStep_Implementation(int index, ANevarokMLEnv* env);\n</code></pre> Implementation of the OnStep event method.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#onreset_implementation","title":"OnReset_Implementation","text":"<p><pre><code>virtual void ANevarokMLTrainer::OnReset_Implementation(int index, ANevarokMLEnv* env);\n</code></pre> Implementation of the OnReset event method.</p>"},{"location":"nevarok-ml/documentation/trainer_api/#oncomplete_implementation","title":"OnComplete_Implementation","text":"<p><pre><code>virtual void ANevarokMLTrainer::OnComplete_Implementation();\n</code></pre> Implementation of the OnComplete event method.</p>"},{"location":"nevarok-ml/documentation/trainer_overview/","title":"NevarokML: ANevarokMLTrainer","text":"<p>The ANevarokMLTrainer class represents a trainer actor in the NevarokML plugin. It is responsible for coordinating the training process and interacting with the environments and algorithms.</p>"},{"location":"nevarok-ml/documentation/trainer_overview/#overview","title":"Overview","text":"<p>The ANevarokMLTrainer class serves as the central component for training machine learning models using the NevarokML plugin. It manages the interaction between the environments, algorithms, and the backend process. The trainer facilitates communication with the environments to collect observations, receive actions, and handle training episodes.</p>"},{"location":"nevarok-ml/documentation/trainer_overview/#key-features-of-the-anevarokmltrainer-class","title":"Key features of the ANevarokMLTrainer class:","text":"<ul> <li>Manages the training process, including environment initialization, stepping, and resetting.</li> <li>Interacts with the backend process for training and inference.</li> <li>Communicates with the environments to collect observations and send actions.</li> <li>Handles events such as initialization, step updates, reset notifications, completion, and errors.</li> <li>Provides hooks for customization and event handling in Blueprint scripts.</li> </ul>"},{"location":"nevarok-ml/documentation/trainer_overview/#api-reference","title":"API Reference","text":"<p>For detailed information on the API of the <code>ANevarokMLTrainer</code> class, refer to the following documentation pages:</p> <ul> <li>Properties</li> <li>Methods</li> <li>Override Methods</li> <li>Event Methods</li> </ul>"},{"location":"nevarok-ml/documentation/trainer_overview/#conclusion","title":"Conclusion","text":"<p>The <code>ANevarokMLTrainer</code> class plays a vital role in coordinating the training process in the NevarokML plugin. It manages the communication between environments, the backend process, and algorithms. By leveraging the customizable event methods and properties, you can tailor the behavior of the trainer to suit your specific training requirements.</p>"},{"location":"nevarok-ml/examples/","title":"Examples and Tutorials","text":"<p>Welcome to the NevarokML examples and tutorials page! Here, you'll find a collection of resources to help you get started with NevarokML, the powerful machine learning plugin for Unreal Engine. Whether you're new to machine learning or an experienced developer, these examples and tutorials will guide you in harnessing the full potential of NevarokML to create intelligent systems within Unreal Engine.</p>"},{"location":"nevarok-ml/examples/#getting-started","title":"Getting Started","text":"<ul> <li>Installation Guide: Learn how to install and set up NevarokML in your Unreal Engine environment.</li> </ul>"},{"location":"nevarok-ml/examples/#tutorials","title":"Tutorials","text":"<ul> <li>Inference and Simulation Tutorial: Discover how to use NevarokML for inference, allowing you to apply your pre-trained machine   learning models within Unreal Engine for intelligent decision-making.</li> <li>Loading Pretrained Models Tutorial: Learn how to load and use pre-trained machine learning models with   NevarokML, enabling you to jumpstart your projects with existing models.</li> <li>Enabling TensorBoard Logging Tutorial: Get insights into how to enable and configure TensorBoard logging for   monitoring and visualizing your machine learning training progress.</li> </ul>"},{"location":"nevarok-ml/examples/#examples","title":"Examples","text":"<ul> <li>Basic Example: Explore a simple example where an agent learns to navigate towards a goal using reinforcement   learning techniques.</li> <li>Basic3D Example: Witness the fundamentals of 3D reinforcement learning as an agent tackles complex navigation   challenges in a 3D environment.</li> <li>Balance Example: Dive into the intricacies of balance control as an agent strives to keep a ball stable.</li> <li>MoveBall Example: Learn how an agent must move a ball to its target using reinforcement learning techniques.</li> <li>MoveBall2P Example: Experience the excitement of competitive ball movement as two agents compete to reach their   respective targets.</li> </ul> <p>We hope these examples and tutorials help you unlock the immense potential of NevarokML in creating intelligent virtual experiences within Unreal Engine. Start exploring, experimenting, and innovating with NevarokML today!</p>"},{"location":"nevarok-ml/getting-started/","title":"NevarokML: Getting Started with Reinforcement Learning","text":"<p>To start exploring reinforcement learning with NevarokML, here are some steps you can follow:</p> <ul> <li> <p>Familiarize Yourself: Gain a solid understanding of reinforcement learning concepts, algorithms, and frameworks. Learn about Markov Decision Processes (MDPs), value functions, policy gradients, and deep reinforcement learning techniques.</p> </li> <li> <p>Install NevarokML: Follow the installation instructions for NevarokML and set up the plugin within your Unreal Engine project.</p> </li> <li> <p>Learn the Basics: Dive into introductory tutorials and resources on reinforcement learning. Understand the fundamentals of training agents, defining reward structures, and implementing reinforcement learning algorithms.</p> </li> <li> <p>Experiment with Environments: Create simple environments within Unreal Engine to train and test your reinforcement learning agents. Start with basic tasks and gradually increase the complexity of the environments to challenge your agents.</p> </li> <li> <p>Work with NevarokML Examples: Explore the example projects and tutorials provided with NevarokML. Gain hands-on experience by working through these examples and understanding how reinforcement learning can be applied in different scenarios.</p> </li> <li> <p>Extend Your Knowledge: Explore advanced topics in reinforcement learning, such as hierarchical reinforcement learning, multi-agent reinforcement learning, and imitation learning. Stay updated with the latest research and advancements in the field.</p> </li> <li> <p>Join the Community: Engage with the reinforcement learning and NevarokML communities. Participate in forums, online communities, and social media platforms to discuss ideas, share insights, and learn from others' experiences.</p> </li> </ul> <p>Reinforcement learning combined with NevarokML brings unprecedented opportunities for creating intelligent and adaptive systems within Unreal Engine. Embrace the challenges and rewards of reinforcement learning, and join us in shaping the future of machine learning in the world of game development.</p>"},{"location":"nevarok-ml/getting-started/installation/","title":"NevarokML: Installing Plugin","text":""},{"location":"nevarok-ml/getting-started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before installing NevarokML, ensure that you have the following prerequisites:</p> <ul> <li>Unreal Engine v5.2 or later: NevarokML requires Unreal Engine version 5.2 or higher. If you don't have Unreal   Engine   installed, visit the official Unreal Engine website, create an Epic Games account, and download the desired version   from   the available options.</li> </ul> <p>Warning</p> <p>Make sure to install Unreal Engine before proceeding with NevarokML installation. Having Unreal Engine v5.2 or later installed on your computer is essential to ensure compatibility and proper functioning of the NevarokML plugin. Once you have met this prerequisite, you can proceed with the installation process.</p>"},{"location":"nevarok-ml/getting-started/installation/#installation-guide","title":"Installation guide","text":"<p>To install NevarokML, a reinforcement learning plugin for Unreal Engine, follow these steps:</p>"},{"location":"nevarok-ml/getting-started/installation/#method-a","title":"Method \"A\"","text":""},{"location":"nevarok-ml/getting-started/installation/#method-b","title":"Method \"B\"","text":""},{"location":"nevarok-ml/getting-started/installation/#method-github","title":"Method \"GitHub\"","text":""},{"location":"nevarok-ml/getting-started/installation/#step-1-access-unreal-engine-marketplace","title":"Step 1: Access Unreal Engine Marketplace","text":"<ul> <li>Go to the Unreal Engine Marketplace website and log in to your Epic Games   account. Navigate to the NevarokML page.</li> </ul>"},{"location":"nevarok-ml/getting-started/installation/#step-2-download-nevarokml","title":"Step 2: Download NevarokML","text":"<ul> <li>Go to your Epic Games Launcher. Under the \"Library\" section, navigate to the \"   Marketplace\" tab. Find NevarokML in your list of items and click on the \"Install\" button to download the   plugin.</li> </ul>"},{"location":"nevarok-ml/getting-started/installation/#step-3-install-nevarokml-in-unreal-engine","title":"Step 3: Install NevarokML in Unreal Engine","text":"<ul> <li>Open Unreal Engine on your computer. Create a new project or open an existing one. In the Unreal Engine editor, go to   the \"Edit\" menu and select \"Plugins.\" In the Plugins window, navigate to the \"Installed\" section and locate NevarokML.   Ensure that the plugin is enabled by checking the box next to its name.</li> </ul>"},{"location":"nevarok-ml/getting-started/installation/#step-4-verify-backend-setup","title":"Step 4: Verify Backend Setup","text":"<p>To ensure that the backend for NevarokML is properly set up, follow these additional steps:</p> <ul> <li>In the Unreal Engine editor, go to the \"Edit\" menu and select \"Editor Preferences.\"</li> <li>In the Editor Preferences window, navigate to the \"Plugins\" tab and find NevarokML in the list.</li> <li>Click on NevarokML to expand its settings.</li> <li>Under the NevarokML settings, locate the \"Installation\" category.</li> <li>Verify that both options show \"OK: Backend Archive Found\" and \"OK: Backend Executable Found\" messages. If you see   any \"Error\" messages, continue to the next step.</li> </ul> <p> </p> Backend Archive Found and Backend Executable Found <ul> <li>If you see an \"Error: Archive Not Found\" message, try reimporting the package. Make sure the archive file is located   within the NevarokML plugin folder, specifically in /NevarokML/Source. After reimporting, check if   the \"OK\" messages appear. <p> </p> Error: Archive Not Found <ul> <li>If the archive is found but you see an \"Error: Backend Executable Not Found\" message, press the \"Unpack Backend\"   button. This will initiate an automated unpacking process for the backend. Wait for the process to finish.</li> </ul> <p> </p> Error: Backend Not Found <p> </p> Archive Unpacking Process <ul> <li>Once the backend setup is complete and both \"Backend Archive\" and \"Backend Executable\" show \"OK\" messages, you can   proceed to use NevarokML in your Unreal Engine projects.</li> </ul>"},{"location":"nevarok-ml/getting-started/installation/#step-5-start-using-nevarokml","title":"Step 5: Start Using NevarokML","text":"<ul> <li>With NevarokML successfully installed, you can now start utilizing its powerful reinforcement learning capabilities   within your Unreal Engine projects. Refer to the NevarokML documentation, tutorials, and examples to learn how to   train   intelligent agents, optimize decision-making, and import pre-trained models.</li> </ul> <p>Note</p> <p>For further support or assistance, you can visit the NevarokML Contacts page. There, you will find information on how to reach out to the NevarokML team for any questions, issues, or additional guidance you may require.</p> <p>By following these steps, you can install NevarokML from the Unreal Engine Marketplace and begin incorporating reinforcement learning into your Unreal Engine projects. Enjoy exploring the possibilities of training intelligent agents and creating dynamic and adaptive virtual experiences.</p>"},{"location":"nevarok-ml/getting-started/markov-decision-processes/","title":"Markov Decision Processes (MDPs) in Reinforcement Learning","text":"<p>Markov Decision Processes (MDPs) are fundamental mathematical models used in reinforcement learning to formalize decision-making problems. MDPs provide a framework for understanding and solving sequential decision-making tasks in uncertain environments.</p>"},{"location":"nevarok-ml/getting-started/markov-decision-processes/#key-concepts","title":"Key Concepts","text":"<ul> <li> <p>States: MDPs consist of a set of states representing different configurations or conditions of the environment.</p> </li> <li> <p>Actions: Agents can take actions in each state, influencing the subsequent state transitions.</p> </li> <li> <p>Rewards: MDPs assign rewards to agents based on their actions and resulting states. The goal is to maximize the cumulative rewards over time.</p> </li> <li> <p>Transition Probabilities: MDPs define transition probabilities that determine the likelihood of transitioning from one state to another based on the chosen action.</p> </li> <li> <p>Policy: A policy defines the agent's decision-making strategy by specifying the action to take in each state.</p> </li> </ul>"},{"location":"nevarok-ml/getting-started/markov-decision-processes/#solving-mdps","title":"Solving MDPs","text":"<p>The objective in solving MDPs is to find an optimal policy that maximizes the expected cumulative reward over time. Several algorithms and techniques can be applied to solve MDPs, including:</p> <ul> <li> <p>Value Iteration: An iterative algorithm that updates the values of states until convergence to find the optimal value function.</p> </li> <li> <p>Policy Iteration: An iterative algorithm that alternates between policy evaluation and policy improvement steps to converge on an optimal policy.</p> </li> <li> <p>Q-Learning: A model-free algorithm that learns the optimal action-value function by interacting with the environment and updating Q-values.</p> </li> <li> <p>Monte Carlo Methods: Sample-based methods that estimate value functions by averaging returns from sample trajectories.</p> </li> </ul>"},{"location":"nevarok-ml/getting-started/markov-decision-processes/#applications-of-mdps","title":"Applications of MDPs","text":"<p>MDPs have various applications in reinforcement learning and beyond, including</p> <ul> <li> <p>Robotics: MDPs enable robots to make intelligent decisions and navigate complex environments.</p> </li> <li> <p>Game AI: MDPs form the basis for developing intelligent agents in video games, capable of adaptive and strategic behavior.</p> </li> <li> <p>Resource Management: MDPs help optimize resource allocation and scheduling in various domains, such as energy management and transportation.</p> </li> <li> <p>Finance: MDPs are used to model and optimize investment and portfolio management strategies.</p> </li> </ul> <p>In conclusion, Markov Decision Processes (MDPs) provide a formal framework for modeling and solving decision-making problems in reinforcement learning. By understanding the concepts and algorithms associated with MDPs, developers can design intelligent agents that make optimal decisions in uncertain environments.</p>"},{"location":"nevarok-ml/getting-started/reinforcement-learning/","title":"What is Reinforcement Learning","text":"<p>Reinforcement Learning is a powerful branch of machine learning that focuses on training intelligent agents to make sequential decisions in an environment to maximize a cumulative reward. It is widely used in various domains, including robotics, game playing, recommendation systems, and autonomous vehicles. With its ability to learn from interactions and adapt to dynamic environments, reinforcement learning has proven to be an effective approach for creating intelligent and adaptive systems.</p>"},{"location":"nevarok-ml/getting-started/reinforcement-learning/#how-reinforcement-learning-works","title":"How Reinforcement Learning Works","text":"<p>Reinforcement learning revolves around an agent interacting with an environment. The agent takes actions based on its current state, and the environment responds by providing feedback in the form of rewards or penalties. The goal of the agent is to learn an optimal policy\u2014a mapping from states to actions\u2014that maximizes the long-term cumulative reward.</p> <p>The reinforcement learning process typically involves the following key components:</p> <ul> <li> <p>Agent: The entity that learns and takes actions in the environment. The agent's objective is to maximize the   cumulative   reward it receives.</p> </li> <li> <p>Environment: The external system with which the agent interacts. It provides the agent with feedback in the form   of   rewards or penalties based on the agent's actions.</p> </li> <li> <p>State: The current representation of the environment that the agent perceives. The state can be a complete   snapshot of   the environment or a partial observation.</p> </li> <li> <p>Action: The decision made by the agent based on its current state. The action can have short-term consequences and   impact future states and rewards.</p> </li> <li> <p>Reward: The feedback signal provided by the environment to the agent. The reward indicates the desirability or   quality   of an action taken by the agent in a given state.</p> </li> </ul> <pre><code>sequenceDiagram\n    autonumber\n    loop until Termination Condition\n        Agent-&gt;&gt;Environment: Take Action\n        Environment--&gt;&gt;Agent: Provide Feedback (Rewards/Penalties)\n        Agent-&gt;&gt;Agent: Update Policy\n    end\n    Agent--&gt;&gt;Agent: Maximize Cumulative Reward</code></pre> <p>The reinforcement learning process involves the agent learning from trial and error. By exploring different actions and observing their consequences, the agent adjusts its policy to maximize the expected cumulative reward.</p>"},{"location":"nevarok-ml/getting-started/reinforcement-learning/#applications-of-reinforcement-learning","title":"Applications of Reinforcement Learning","text":"<p>Reinforcement learning has a wide range of applications across various domains. Some notable examples include:</p> <ul> <li> <p>Game Playing: Reinforcement learning has achieved remarkable success in mastering complex games such as Go, Chess,   and   video games. Agents trained through reinforcement learning have surpassed human-level performance in these domains.</p> </li> <li> <p>Robotics: Reinforcement learning enables robots to learn complex tasks and adapt to dynamic environments. It has   been   used in robot locomotion, manipulation, and autonomous navigation.</p> </li> <li> <p>Recommendation Systems: Reinforcement learning techniques have been employed in recommendation systems to optimize   recommendations for users, maximizing their engagement and satisfaction.</p> </li> <li> <p>Autonomous Vehicles: Reinforcement learning plays a crucial role in training autonomous vehicles to make   intelligent   decisions in real-world scenarios, such as lane changing, traffic signal control, and path planning.</p> </li> <li> <p>Resource Management: Reinforcement learning has been used in optimizing resource allocation and management, such   as   energy management in smart grids and traffic signal optimization.</p> </li> </ul> <p>Reinforcement learning offers an exciting and powerful approach to training intelligent agents that can adapt and learn from their environment. Embrace the challenges and rewards of reinforcement learning and unlock the potential of intelligent decision-making in your projects.</p>"},{"location":"nevarok-ml/getting-started/supported-platforms/","title":"NevarokML: Supported Platforms","text":"<p>NevarokML, the powerful machine learning plugin for Unreal Engine, supports various platforms for deploying your AI-driven projects. With NevarokML, you can create intelligent and adaptive virtual experiences that can be enjoyed by users on different devices and operating systems. Here is a list of supported platforms for NevarokML:</p> <p>Windows: NevarokML supports Windows as the primary platform for training machine learning models. You can leverage the full range of NevarokML's training capabilities on Windows machines.</p> <p>Unreal Engine Supported Platforms: Once the models are trained, they can be imported into Unreal Engine and used on any platform supported by Unreal Engine itself. For more information about supported platforms please refer to Official Unreal Engine NNE documentation</p> <p>NevarokML's compatibility with Unreal Engine's supported platforms allows you to deploy your AI-powered projects across multiple devices and operating systems, ensuring a wider reach for your applications.</p> <p>Note</p> <p>Please note that while training models is currently supported only on Windows machines, the trained models can be imported and utilized on any platform supported by Unreal Engine's Neural Network Engine (NNE). This provides flexibility in developing and deploying your machine learning projects across various platforms.</p> <p>Note</p> <p>It's important to refer to the specific documentation and guidelines provided by Unreal Engine for each platform to ensure compatibility and optimize performance when using NevarokML on different devices.</p> <p>Harness the power of NevarokML and Unreal Engine to create compelling and intelligent virtual experiences on a range of supported platforms. Expand your audience and deliver immersive AI-driven applications to users worldwide.</p>"},{"location":"nevarok-ml/getting-started/under-the-hood/","title":"NevarokML: Under the Hood","text":"<p>NevarokML is a powerful plugin designed to bring machine learning capabilities to Unreal Engine. It operates as a client-server application, where Unreal Engine serves as the server, and a Python library acts as the client. This setup enables seamless integration of machine learning functionalities into Unreal Engine projects. Here's an overview of how NevarokML works:</p> <pre><code>sequenceDiagram\n  autonumber\n  participant Unreal Engine\n  participant NevarokML Plugin\n  participant Python Library\n\n  Unreal Engine -&gt;&gt; NevarokML Plugin: Send game state and observations\n  NevarokML Plugin -&gt;&gt; Python Library: Receive game state and observations\n  Python Library -&gt;&gt; NevarokML Plugin: Perform machine learning computations\n  NevarokML Plugin -&gt;&gt; Unreal Engine: Send computed results (actions, model parameters)</code></pre>"},{"location":"nevarok-ml/getting-started/under-the-hood/#client-server-architecture","title":"Client-Server Architecture:","text":"<ul> <li>NevarokML follows a client-server architecture, with Unreal Engine as the server and the Python library as the client. This architecture facilitates the integration of machine learning capabilities into Unreal Engine projects.</li> </ul>"},{"location":"nevarok-ml/getting-started/under-the-hood/#communication-via-sockets-and-json","title":"Communication via Sockets and JSON:","text":"<ul> <li>NevarokML utilizes sockets and exchanges data in JSON format to enable communication between Unreal Engine and the Python client. This approach ensures efficient and reliable transfer of information between the server and the client.</li> </ul>"},{"location":"nevarok-ml/getting-started/under-the-hood/#training-models","title":"Training Models:","text":"<ul> <li>NevarokML supports training machine learning models using reinforcement learning algorithms. It leverages the stable-baselines3 library, a popular and reliable reinforcement learning library. With NevarokML, you can use algorithms such as Proximal Policy Optimization (PPO), Advantage Actor-Critic (A2C), Deep Deterministic Policy Gradient (DDPG), Deep Q-Network (DQN), Soft Actor-Critic (SAC), and Twin Delayed DDPG (TD3) to train intelligent agents.</li> </ul>"},{"location":"nevarok-ml/getting-started/under-the-hood/#integration-with-unreal-engine-neural-network-engine-nne","title":"Integration with Unreal Engine Neural Network Engine (NNE):","text":"<ul> <li>NevarokML seamlessly integrates with Unreal Engine's Neural Network Engine (NNE). It allows you to import trained models into Unreal Engine in the form of NNEModelData. These models can enhance the capabilities of Unreal Engine projects, enabling AI-driven experiences.</li> </ul> <p>Note</p> <p>It's important to note that the models are trained in the stable-baselines3 environment and are automatically converted to ONNX models upon saving. Unreal Engine supports ONNX models, ensuring compatibility and ease of use.</p>"},{"location":"nevarok-ml/getting-started/under-the-hood/#platform-compatibility","title":"Platform Compatibility:","text":"<ul> <li>While the training of models is currently supported only on Windows machines, the trained models can be imported and used on any platform supported by Unreal Engine.</li> </ul> <p>By leveraging the capabilities of NevarokML, Unreal Engine developers can harness the power of machine learning algorithms and create immersive and intelligent experiences within their projects.</p>"},{"location":"nevarok-ml/introduction/nevarokml-reinforcement-learning/","title":"How NevarokML Empowers Reinforcement Learning in Unreal Engine","text":"<p>NevarokML seamlessly integrates the power of reinforcement learning into the Unreal Engine ecosystem. By leveraging the stable-baselines3 library, NevarokML provides developers with a robust set of reinforcement learning algorithms, including PPO, A2C, DDPG, DQN, SAC, and TD3.</p> <p>With NevarokML, developers can:</p> <ul> <li> <p>Train Intelligent Agents: Create intelligent agents within Unreal Engine that can learn and adapt to dynamic   environments through reinforcement learning techniques.</p> </li> <li> <p>Optimize Decision-Making: Utilize reinforcement learning algorithms to train agents to make optimal decisions   based on   their current states and maximize cumulative rewards.</p> </li> <li> <p>Deepen Learning Capabilities: Combine reinforcement learning with deep neural networks to handle complex tasks and   achieve higher levels of performance.</p> </li> <li> <p>Import Pre-Trained Models: NevarokML supports the import of pre-trained models in the form of NNEModelData, which   seamlessly integrates with Unreal Engine's Neural Network Engine (NNE).</p> </li> </ul> <p>This plugin opens up exciting possibilities for creating adaptive and intelligent environments within Unreal Engine, pushing the boundaries of what can be achieved in the field of reinforcement learning. Unlock the full potential of your projects with NevarokML and revolutionize the way you approach machine learning in Unreal Engine.</p>"},{"location":"nevarok-ml/introduction/what-is-nevarokml/","title":"What is NevarokML?","text":"<p>NevarokML is a powerful plugin designed to bring machine learning capabilities to Unreal Engine. Built upon the stable-baselines3 library, NevarokML extends its functionality and provides developers with the tools to harness the power of reinforcement and deep reinforcement learning within their Unreal Engine projects. Whether you prefer working with C++ or Blueprints, NevarokML offers a seamless integration that allows you to leverage the potential of machine learning algorithms.</p>"},{"location":"nevarok-ml/introduction/what-is-nevarokml/#key-features","title":"Key Features","text":"<ul> <li> <p>Reinforcement Learning: NevarokML enables you to incorporate reinforcement learning techniques into your Unreal   Engine   projects. By utilizing algorithms such as Proximal Policy Optimization (PPO), Advantage Actor-Critic (A2C), Deep   Deterministic Policy Gradient (DDPG), Deep Q-Network (DQN), Soft Actor-Critic (SAC), and Twin Delayed DDPG (TD3), you   can train intelligent agents to interact and learn from their environment.</p> </li> <li> <p>Stable-baselines3 Compatibility: NevarokML inherits functionality from the stable-baselines3 library, a popular   and   reliable library for reinforcement learning. By building upon stable-baselines3, NevarokML benefits from its robust   algorithms and proven techniques, ensuring stability and reliability in your machine learning workflows.</p> </li> <li> <p>Integration with Neural Network Engine (NNE): NevarokML seamlessly integrates with Unreal Engine's Neural Network   Engine (NNE). It provides support for importing models into Unreal Engine in the form of NNEModelData. This enables   you   to utilize pre-trained models and leverage their capabilities within your Unreal Engine projects, opening up a world   of   possibilities for AI-driven experiences.</p> </li> </ul>"},{"location":"nevarok-ml/introduction/what-is-nevarokml/#getting-started","title":"Getting Started","text":"<p>To get started with NevarokML, you can follow these steps:</p> <ul> <li> <p>Installation: Download and install the NevarokML plugin from the official website or the Unreal Marketplace. Follow the provided instructions to integrate the plugin into your Unreal Engine project.</p> </li> <li> <p>Documentation: Refer to the comprehensive documentation provided by NevarokML to understand the plugin's features, functionalities, and usage instructions. The documentation covers everything from setting up your environment to training and deploying machine learning agents within Unreal Engine.</p> </li> <li> <p>Examples and Tutorials: Explore the range of examples and tutorials available with NevarokML to gain hands-on experience and learn best practices. These resources provide step-by-step guidance on implementing machine learning algorithms, training agents, and integrating them into your Unreal Engine projects.</p> </li> <li> <p>Community and Support: Join the NevarokML community on Discord, where you can connect with other users, share your experiences, ask questions, and receive support. Engage in discussions, exchange ideas, and collaborate on projects to enhance your machine learning endeavors.</p> </li> </ul> <p>Start unlocking the potential of machine learning in Unreal Engine with NevarokML. Empower your projects with intelligent agents and create immersive experiences driven by AI.</p>"}]}